Analyze text with the Language service:

    - The language service is a part of the Azure Cognitive Services offerings that can perform 
    advanced natural language processing over raw text

    - The Language Cognitive Service can help simplify application development by using pre-trained
    models that can:
        - Determine the language of a document or text (for ex. French or English). Submit multiple
        Documents at a time. Returns the following output:
            - The language name (ex. English)
            - The ISO 639-1 language code (ex. "en")
            - A score indicating a level of confidence in the language detection
        - Perform Sentiment Analysis on text to determine a positive or negative Sentiment
            - Score between 0 to 1
            - Close to 1 is a positive Sentiment
            - Close to 0 is a negative Sentiment
            - Close to 0.5 is a neutral sentiment 
        - Extract Key phrases from text that might indicate its main talking points
            - Used to identify important elements of the review   
        - Identify and categorize entities in the text. Entities can be people, places, organizations
        or even everyday items such as dates, times, quantities and so on

    - The Language Service can be provisioned through a:
        - Language Resource: 
            - For exclusive access and managing and billing separately
        - Cognitive Service Resource
            - If you want to pair this service with other cognitive services and managing and billing
            togather
    
    - Language Detection:
        - For a sentence having text with mixed languages, the language detection service will focus on 
        the predominant language in the text
        - The service uses an algorithm to determine the predominant language, such as length of phrases 
        or total amount of text for the language compared to other languages in the text
        - The predominant language will be the value returned, along with the language code
        - The confidence score may be less than 1 as a result of the mixed language text
        - For ambitious and mixed language content like ":-)", it results in a value of "unknown" 
        for the language name and the language identifier, and a score of NaN
    
    - Sentiment Analysis:
        - A list of words in a sentence that has no structure could result in an indeterminate score
        - Also if the content is in some different language and the language is passed different, 
        that will also generate an indeterminate score
    
    - Entity Recognition:
        - It is essentially an item of a particular type or a category and in some cases, subtype
        - The service also supports entity linking to help disambiguate entities by linking to a 
        specific reference. For recognized entities, the service returns a URL for a relevant 
        Wikipedia article
    
Recognize and Synthesize Speech:
    - Speech Recognition: 
        - The ability to detect and interpret spoken input
        - To accomplish this feat, the software typically uses multiple types of models, including:
            - An accoustic Model that converts the audio signal into phenomes (representations
            of specific sounds)
            - A language model that maps phenomes into word, usually using a statistical algorithm
            that predicts the most probable sequence of words based on the phenomes
        - The recognized words are typically converted to text for the purpose of:
            - Providing closed captions for recorded or live videos
            - Creating a transcript of a phone call or meeting
            - Automated note dictation
    
    - Speech Synthesis:
        - The ability to generate spoken output
        - It requires typically the following information:
            - The text to be spoken
            - The voice to be used to vocalize the speech 
        - Use cases includes:
            - Generating spoken responses to user input
            - Creating Voice menus for telephone systems
            - Reading Email or text messages aloud in hands-free scenarios
            - Broadcasting announcements in public locations, such as railway stations or airports
        
    - To use the Speech Service in an application, you must create an appropriate resource in Azure
    subscription:
        - A Speech resource
         - For exclusive access and managing and billing separately
        - Cognitive Service Resource
            - If you want to pair this service with other cognitive services and managing and billing
            togather
    
    - Speech to Text API:
        - Perform real-time or batch transcription of audio into a text format
        - The model for this API is built on the Universal Language Model that was trained by 
        Microsoft
        - The model is optimized for two scenarios, conversational and dictation
        - You can create your own custom models including accoustics, language, and pronounciation
        if the pre-built models from Microsoft do not provide what you need
        - Speech to text Real Time:
            - In order for a real time transcription to work, your application need to be listening 
            for incoming audio from a microphone or other input source such as an audio file. 
            - The application code streams the audio to the service, which returns the transcribed 
            text
        - Speech to Text for Batch Transcription:
            - You may have audio recordings on a file share, a remote server, or even on Azure 
            storage
            - You can point to audio files with a Shared Access Signature (SAS) URI and 
            synchronously recieve transcription results
            - Normally a job will start executing within minutes of the request but there is no 
            estimate for when a job changes into the running state
    
    - The text to speech API:
        - It enables you to convert text input to audible speech, which can either be played 
        directly through a computer speaker or written to an audio file
        - The service includes multiple pre-defined voices with support for multiple languages
        and regional pronunctiation, including standard voices as well as neural voices
        - Neural voices leverage neural networks to overcome common limitations in speech 
        synthesis with regard to intonation, resulting in a more natural sounding voice
        - You can also develop custom voices and use them with the text to speech API
    
Translate Text to Speech:
    - A translation service must take into account the semantic context and return a translation
    and not just apply literal translation (word to word)
    - Text Translation can be used to translate documents from one language to another, translate
    email communications that come from foreign governments or translate a webpage
    - Speech Translation is used to translate between spoken language, sometimes directly 
    (speech-to-speech translation) and sometimes by translating to an intermediary text format 
    (speech-to-text translation)

    - To use the Speech Service in an application, you must create an appropriate resource in Azure
    subscription:
        - A Translator or Speech resource
         - For exclusive access and managing and billing separately
        - Cognitive Service Resource
            - If you want to pair this service with other cognitive services and managing and billing
            togather
    
    - Translator Service:
        - Supports Text-to-Text translation
        - This service uses a neural machine translation model for translation
        - You must specify the language you are translating from and the language you are translating
        to using ISO 639-1 language codes, such as en for english, fr for french, etc
        - Alternately you can specify cultural variants of languages by extending the language code 
        with the appropriate 3166-1 cultural code - for example, en-US for US english, en-GB for
        Great Britain or fr-CA for Canadian French
        - You can also specify one from language with multiple to language enabling you to
        simultaneously translate a source document into multiple languages
        - Optional Configurations includes:
            - Profanity Filtering:
                - By marking the translated text as profane or by omitting it in the results
            - Selective Translation
                - You may tag a content that isn't translated

    - Speech Service:
        - Supports Speech to text and speech-to-speech translation
        - This service includes the following APIs:
            - Speech to Text:
                - Transcribe speech from an audio source to text format
            - Text to Speech
                - Generate spoken audio from a text source
            - Speech Translation 
                - Translate speech in one language to text or speech in another
        - Use the Speech Translation API to translate spoken audio from a streaming source, 
        such a microphone or audio file, and return the translation as text or an audio stream
        - You can specify one source language and one or more target language to which the source
        should be translated
        - The source language must be specified using the extended language and culture code 
        format, ex: es-US for American Spanish
        - The target languages must be specified using a two-character language code, 
        ex: en for English

Create a Language Model with Conversational Language Understanding:
    - Conversational language understanding is supported through the Language Service
    
    - You need to take into account three concepts:
        - Utterances:
            - Example of something a user might say, and which your application must interpret
            - Ex: "Switch the fan on", "Turn on the light"
        - Entities:
            - An entity is an item to which an utterance refers
            - Ex. Fan and Light in the above Utterance
        - Intents
            - Represents the purpose or goal expressed in a user's utterance
            - The intent is to turn on a device for both the utterance
            - Hence in this case, you might define a TurnOn intent that is related to these Utterances
    
    - Utterances are used to train the model to identify the most likely intent and entities to which
    it should be applied based on a given input

    - None Intent:
        - You should consider always using the None intent to help handle utterances that do not map any
        of the utterances you have entered. 
        - It is considered a fallback, and is typically used to provide a generic response to users when 
        their requests don't match any other intent. 
        - It is a required intent and cannot be deleted or renamed
        - Fill it with utterances that are outside of your domain
    
    - After defining the entities and intents with sample utterances in Conversational Language 
    Understanding (CLU) application, you can train a language model to predict intents and entities 
    from user input, even if it does not match the sample utterances exactly

    - Creating CLU consists of two main tasks:  
        - Authoring:
            - Define entities, intents and utterances with which to train the language model
            - CLU provides a comprehensive collection of prebuilt domains that include 
            pre-defned intents and entities for common scenarios; which you can use as a starting
            point for your model
            - You can also create your own entities and intents
            - You can write code to define elements of your model or use the CLU portal for creating
            and managing Conversational Language Understanding
            
            - Intents:
                - Define Intents based on actions a user would want to perform with your application
                - Include a variety of utterances for each intent that provides examples of how a 
                user might express the intent
                - If an intent can be applied to multiple entities, include sample utterances for
                each potential entity and ensure that each entity is identified in the utterance
            
            - Entities:
                - Machine Learned:
                    - Entities that are learned by your model during training from context in the 
                    sample utterances you provide
                - List:
                    - Entities that are defined as a heirarchy of lists and sublists. Ex Device 
                    list includes sublists such as lamp, fan etc. Synonyms can be added - light (lamp)
                - Regex:
                    - Describing a pattern. Ex. [0-9]{3}-[0-9]{3}-[0-9]{4} signifying telephone number
                - Pattern.any:
                    - Entities that are used with patterns to define complex entities that may be hard
                    to extract from sample utterances 
            
            - Training:
                - It is the process of using your sample utterances to teach your model to match 
                natural language expressions that a user might say to probable intents and entities
                - Post training, test your model by submitting text and reviewing the predicted intents

        - Prediction:
            - Publish the model so that client applications can use it for intent and entity    
            prediction based on user input
            - The predictions (intents and entities) are returned to the client applications
            which can take appropriate action based on the predicted intent 
 
    
    - Resources for Conversational Language Understanding:
        - Language Service:
        - Cognitive Service:
            - This service can be used only for prediction