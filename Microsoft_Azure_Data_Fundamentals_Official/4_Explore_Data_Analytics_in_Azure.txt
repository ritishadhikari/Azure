Explore Fundamentals of Large Scale Data Warehousing
    - Big Data processing solutions are used with large volumes of data in multiple
    formats, which is batch loaded or captured in real-time streams and stored in 
    a data lake from which distributed processing engines like Apache Spark are used
    to process it.

    - Data Warehousing Architecture:
        - Data Ingestion and Processing:
            - Data from one or more transactional data stores, files, real-time 
            streams, or other sources is loaded into a data lake or a relational
            data warehouse
            - The load operation usually involves ETL or ELT
            - The resulting data structure is optimized for analytical queries
            - Data processing is often performed by distributed systems that can 
            process high volumes of data in parallel using multi-node clusters
            - Data Ingestion includes both batch processing of static data and real-time
            processing of streaming data
        - Analytical Data Store:
            - Data Stores for Large scale analytics
            - Includes relational data warehouse or file-system based data lakes
            - Also may include combined features of data warehouses and data lakes
            (sometimes called data lakehouses or lake databases)
        - Analytical Data Model:
            - It is common to create one or more data models that pre-aggregate the data
            to make it easier to produce reports, dashboards and interactive 
            visualizations
            - Often these models are called cubes, in which numeric data values are 
            aggregated across one or more dimensions
        - Data Visualization:
            - Data Analysts consume data from analytical models, and directly from 
            analytical stores to create reports, dashboards and other visualizations
    
    - Explore Data Ingestion Pipelines:
        - Pipeline consists of one or more activities that operate on data
        - In Azure, Large-Scale data ingestion is best implemented by creating pipelines
        that orchestrate ETL Processes
        - Create and run pipelines using Azure Data Factory or you can use the same 
        pipeline in Azure Synapse Analytics if you want to manage all of the components
        of your data warehousing solution in a unified workspace
        - An input dataset provides the source data and activities can be defined as a 
        data flow that incrementally manipulates the data until an output dataset
        is produced
        - Pipeline use linked services to load and process data - enabling you to use
        the right technology for each step of the workflow
        - Ex of Linked Services:
            - Azure Blob Store Linked Service
            - Azure SQL Database Linked Service
            - Azure Databricks or Azure HDInsight Linked Service
            - Azure Synapse Analytics Linked Service
        - Pipelines can also include some built-in activities, which don't require 
        a linked service

    - Data Warehouses:
        - Relational Database, where the data is stored in a schema that is optimized
        for data analytics rather than transactional workloads
        - Commonly, the data from a transactional store is transformed into a schema 
        in which numeric values are stored in central fact tables, which are related 
        to one or more dimension tables that represent entities by which the data can 
        be aggregated
    
    - Data Lake:
        - It is a file store, typically on a distributed file system for high performance
        data access
        - Technologies like Spark or Hadoop are often used to process queries on the 
        stored files and return data for reporting and analytics
        - Often applies a schema on read approach to define tabular schemas on 
        semi structured data files where the data is read for analysis, without
        applying write schema contraints when it is stored
    
    Hybrid Approach:
        - Combines features of Data Warehouses and Data Lakes in a lake database
        or data lakehouse
        - Raw data is stored as files in a data lake and a relational storage layer
        abstracts the underlying files and expose them as tables, which can be queried
        using SQL
        - SQL pools in Azure Synapse includes Polybase which enables you to define 
        external tables based on files in a datalake (andother sources) and query 
        them using SQL
        - Synapse Analytics also supports a Lake Database approach in which you can use
        database templates to define the relational schema of your datawarehouse
        while storing underlying data in data lake storage
        - This separates the storage and compute for your data warehousing solution
        - Data Lakehouses are a relatively new approach in Spark based systems and
        are enabled through Delta Lake Technologies
        - It adds relational storage capabilities to spark so that you can define tables
        that enforce schemas and transaction consistency, support batch-loaded and 
        streaming data sources and provide a SQL API for querying
    
    - Azure Service for Analytical Stores:
        - Azure Synapse Analytics:
            - Unified end-to-end solution for large scale data analytics
            - Combines the data integrity and reliability of a scalable, high-performance
            SQL server based relational data warehouse with the flexibility of data lake
            and open-source Apache Spark
            - Includes Native support for log and telemetry analytics with Azure Synapse
            data explorer tool 
            - Includes built in Data Pipelines for data ingestion and transformation
            - All Azure Synapse analytics can be managed through a single, interactive 
            user interface called Azure Synapse Studio which includes the ability to 
            create interactive notebooks in which spark code and markdown content
            can be combined
            - Great choice when you want to create a single, unified analytics solution
            on Azure

        - Azure Databricks:
            - Azure Implementation of Databricks Platform
            - Provides an interactive user interface through which the system can be 
            managed and data can be explored in interactive notebooks
            - Consider using Azure Databricks as your analytical store if you want to 
            use existing expertise with the platform or if you need to operate in a 
            multi-cloud environment or support a cloud-portable solution
        
        - Azure HDInsight:
            - Azure Service that supports multiple open-source data analytics 
            cluster types
            - Not as user friendly as Azure Synapse Analytics and Azure Databricks
            - Can be suitable if your analytics solution relies on multiple open-source
            frameworks or if you need to migrate an existing on-premises Hadoop-based
            solution to the cloud
    
    - A pipeline might use a MapReduce job running in HDInsight or a notebook running 
    in Azure Databricks to process a large volume of data in the data lake and 
    then load it into tables in a SQL pool in Azure Synapse Analytics

Explore Fundamentals of Real Time Analytics:
    - In stream processing, a source is constantly monitored and processed in real time
    as new data events occur
    - Stream Processing is ideal for time-critical operations that require an instant
    real-time response
    - Stream processing is intended for individual records or micro batches consisting
    of few records
    - Stream Processing typically occurs immediately, with latencies in the order of
    seconds or milliseconds
    - Stream Processing is used for simple response functions, aggregates, or
    calculations such as rolling averages
    - It's common for stream processing solutions to capture real-time data, process it
    by filtering or aggregating it and present it in a real time dashboards and 
    visualizations, while also persisting the processed results in a data store
    for historical analysis alongside batch processed data

    - Technologies for Real Time Analytics with streaming data in Azure:
        - Azure Stream Analytics:
            - A PaaS service, that you can use to define streaming jobs that ingest
            data from a streaming source, apply a perpetual query and write the results
            to an output
        - Spark Structured Streaming:
            - An open-source library that enables you to develop complex streaming
            solutions on Apache Spark based Services
            - Includes Azure Synapse Analytics, Azure Databricks and Azure HDInsight
        - Azure Data Explorer:
            - A high-performance database and analytics service
            - Optimized for ingesting and querying batch or streaming data with a
            time-series element
            - Can be used as a standalone Azure service or as an Azure Synapse Data 
            Explorer runtime in an Azure Synapse Analytics workspace
    
    - Source for Stream Processing:
        - Azure Event Hubs:
            - A data ingestion service, that you can use to manage queues of event data,
            ensuring that each event is processed in order, exactly once
        - Azure IoT Hub:
            - Data Ingestion service similar to Event Hubs
            - Optimized for managing event data from IoT devices
        - Azure Data Lake Storage (Gen 2):
            - Highly scalable storage service that is often used in batch processing 
            scenarios, but can also be used as a source of streaming data
        - Apache Kafka:
            - Open Source data ingestion solution that is commonly used togather 
            with Apache Spark
            - Use Azure HDInsight to create a Kafka cluster
    
    - Sinks for Stream Processing:
        - Azure Event Hubs:
            - Used to stream the processed data for further downstream processing
        - Azure Data Lake Store Gen2 or Azure Blob Storage:
            - Used to persist the processed results as a file
        - Azure SQL Database or Azure Synapse Analytics or Azure Databricks:
            - Used to persist the processed results in a database table for querying
            and analysis
        - Microsoft Power BI:
            - Used to generate real time data visualizations in reports and dashboards
    
    - Azure Stream Analytics (ASA):   
        - It is a service for complex event processing and analysis of streaming data
        - It is used to ingest data from an input, such as an Azure Event Hub, Azure
        IOT Hub or Azure Storage Blob Container
        - Then process the data by using a query to select, project and aggregate data
        values
        - Finally write the results to an output, such as Azure Data Lake Gen 2, Azure
        SQL Database, Azure Synapse Analytics, Azure Functions, Azure Event Hub,
        Microsoft Power BI or others
        - Once started, a Stream Analytics query will run perpetually, processing
        new data as it arrives in the input and storing results in the output
        - The easiest way to use ASA is to create Stream Analytics Job in an 
        Azure Subscription, configure its input(s) and output(s) and define the 
        query that the job will use to process the data
        - The query used is SQL and can incorporate statistics reference data from 
        multiple data sources to supply lookup values that can be combined with the 
        streaming data ingested from an input
        - Create a Stream analysis cluster if your stream process requirements are
        complex or resource-intensive; it also uses the same underlying processing
        engine as a Stream Analytics job, but in a dedicated tenant and with configurable
        scalability that enables you to define the right balance of throughput and cost
        for your specific scenario
    
    - Spark Structured Streaming (SSS):
        - It is a great choice for real time analytics when you need to incorporate
        streaming data into a spark based data lake or analytical data store
        - To process Streaming Data on Spark, you can use the Spark Structured Streaming
        library which provides an API for ingesting and outputting results from 
        perpetual streams of data
        - SSS in built on a ubiquitous structure in Spark called a Dataframe, which
        encapsulates a table of data
        - Use SSS streaming API to read data from a real time data source, such as 
        Kafka hub, a file store or a network port into a boundless dataframe that 
        is continually populated with the new data from the stream
        - Define a query on the dataframe that selects or aggregates the data often 
        in temporal windows
        - The result of the aggregate query generates another dataframe which can be
        persisted for analysis for further processing
    
    - Delta Lake:
        - It is an open source storage layer, that supports transactional consistency, 
        schema enforcement and other common data warehousing features to data lake 
        storage
        - Unifies storage for streaming and batch data and can be used in spark to 
        define relational tables for both batch and stream processing
        - When used for stream processing, a delta lake table can be used as a streaming
        source for queries against real-time data, or as as sink to which a stream 
        of data is written
        - Spark runtimes in Azure Synapse Analytics and Azure Databricks include
        support for Delta Lake
        - Delta Lake combined with SSS is a good solution when you need to abstract
        batch and stream processed data in a data lake behind a relational schema
        for SQL based querying and analysis
    
Explore Fundamentals of Data Visualization:
    - A typical workflow for creating a data visualization solution starts with
    Power BI desktop in which you can import data from a wide range of data sources,
    combine and organize the data from these sources in an analytics data model,
    and create reports that contain interactive visualizations of the data
    - After creating data models and reports, you can publish them to the PowerBI 
    service, which is a cloud service where post publishing, reports can be interacted
    by business users
    - Functionality for report editing and data modelling is limited in PowerBI 
    service
    - You can use the service to schedule refreshes of the data sources on which 
    your reports are based and to share reports with other users
    - Apart from the Power BI service, users can also consume reports and dashboards
    on mobile devices by using the Power BI phone App

    - Tables and Schema:
        - Dimension tables represent the entities by which you would want to 
        aggregate numeric measures
        - Each entity is represented by a row with a unique key value while the 
        remaining columns represents attributes of an entity
        - It's most common in most analytical models to include Time dimension so 
        that you can aggregate numeric measures associated with events over time
        - The numeric measures that will be aggregated by the various dimensions in the
        model are stored in fact tables
        - Each row in a fact table represents a recorded event that has numeric measures
        associated with it
        - The type of schema where a fact table is related to one or more dimension
        table is referred to as a star schema
        - You can also define a more complex schema in which dimension tables are
        related to additional tables containing more details and is known as 
        Snowflake schema
        - The schema of fact and dimension tables is used to create an analytical model,
        in which measure aggregations across all dimensions are pre-calculated, making
        performances of analysis and reporting activities much faster than calculating
        the aggregations each time

    - Attribute Hierarchies:
        - Attribute Hierarchies enable you to quickly drill-up or drill-down to find 
        aggregated values at different levels in a hierarchical dimension
        - Ex. In the product table, you can form hierarchy in which each category might
        include named products while in a customer table, a hierarchy could be formed
        to represent multiple named customers in each city
    
    - Analytical Modelling in Microsoft PowerBI:
        - Use PowerBI to define an analytical model from tables of data, which can
        be imported from one or more data source
        - Then use the data modelling interface on the model tab of PowerBI desktop
        to define your analytical model by creating relationships between fact and 
        dimension tables, defining hierarchies, setting data types and display formats
        for fields in the tables and managing other properties of the data that help
        define a rich model for analysis 
    
    - Considerations for Data Visualizations:
        - After you have created a model, you can use it to generate data Visualizations
        that can be included in a report
        - Few types of visualizations included in PowerBI includes:
            - Tables and Text
            - Bar and Column Charts
            - Line Charts
            - Pie Charts
            - Scatter Plots
            - Maps
        - Apart from the built-in visualizations in Power BI, it can be extended to
        custom and third party visualizations
        - In PowerBI, the visual elements for related data in a report are automatically
        linked to one another and provide interactivity
        - For. Ex., selecting an individual category in one visualization will
        automatically filter and highlight that category in other related visualizations
        in the report