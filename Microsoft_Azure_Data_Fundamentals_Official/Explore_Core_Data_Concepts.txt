Explore Core Data Concepts:

    - Structured Data:
        - Adheres to the same schema
        - All of the data has the same fields or properties
        - Represented in tabular format
            - Rows and Columns
        - Often stored in a database in which multiple tables can reference one
        another by using key values in a relational model
    
    - Semi-Structured Data:
        - Information that has some structure
        - Allows for some variation in entity instances
        - JSON is a common format for semi-structured data
    
    - Unstructured Data:
        - Documents, Audio, Images, Video data and Binary Files might
        not have a specific structure
    
    - Data Stores:
        - Stored data can be retrieved for analysis and reporting later
        - Two broad categories of data store in common use:
            - File Stores
            - Databases
    
    - The specific file format used to store data depends on a number of factors:
        - The type of data being stored (structured, semi-structured or unstructured)
        - The applications and services that will need to read, write and process the
        data
        - The need for the files to be readable by humans, or optimized for efficient
        storage and processing

    - Delimited Text Files:
        - Good Choice for structured data that needs to be accessed by a wide range of
        applications and services in a human-readable format

        - Ex:
            - Comma Separated Values (CSV):
                - Fields are separated by Comma
                - Rows are terminated by a carriage return / new line
                - Optionally the first line may include the field names
            - Tab Separated Values (TSV):
            - Space Delimited
            - Fixed width Data:
                - Each field is allocated a fixed number of characters

    - JSON:       
        - A format in which a hierarchical schema is used to define data entities
        (objects) that have multiple attributes
        - Good with both structured and semi-structured data

    - Extensible Mark Up Language (XML):
        - Human Readable which was popular in the 90s and 2000s
        - Uses Tags, enclosed in angle-brackets (<../>) to define elements and 
        attributes

    - Binary Large Objects (BLOBS) :
        - Ultimately all files are stored as binary data (1's and 0's)
        - In the human readable formats, the bytes of binary data are mapped to printable
        characters (typically through a character encoding scheme such as ASCII or
        Unicode)
        - Some file formats, particularly for unstructured data, store the data as raw 
        binary that must be interpreted by applications and rendered
        - Common types of data stored as binary includes:
            - Video
            - Audio
            - Application specific documents
        - When working with data like this, data professionals often refer to the data
        files as BLOBs (Binary Large Objects)

    - Optimized File Formats:
        - Specialized formats that enable compression, indexing and efficient storage
        and processing have been developed
        - Avro:
            - Row based format created by Apache
            - Each record contains a header that describes the structure of the data
            in the record
            - The header is stored as JSON, the data is stored as binary information
            - An application uses the information in the header to parse the binary
            data and extract the fields it contains
            - Avro is a good format for compressing data and minimizing storage and
            network bandwidth requirements
        - Optimized Row Columnar Format (ORC):
            - Organizes data into columns rather than rows
            - Developed by HortonWorks for optimizing read and write operations in 
            Apache Hive
            - An ORC file contains stripes of data, where each stripe holds the data
            for a column or set of columns
            - A stripe contains 
                - An index into the rows in the stripe
                - The data for each row
                - Footer that holds statistical information (count, sum, max, min, etc)
                for each column
        - Parquet:
            - Columnar Data Format
            - Created by Clodera and Twitter
            - A parquet file contains row groups
            - Data for each column is stored togather in the same row group 
            - Each row group contains one or more chunks of data
            - A Parquet file includes metadata that describes the set of rows found 
            in each chunk
            - An application can use this metadata to quickly locate the correct chunk
            for a given set of rows, and retrieve the data in the specified columns
            for these rows
            - Parquet specializes in storing and processing nested data types efficiently
            - It supports very efficient compression and encoding schemes
    
    - Explore Databases:
        - Relational databases are commonly used to store and query structured data
        - Data is stored in tables that represents entities
        - Each instance of an entity is assigned a primary key that uniquely identifies
        it 
        - These keys are used to reference the entity instances in other tables
        - The use of keys to reference data entities enables a relational database
        to be normalized, which in part means the elimination of duplicate data values
        so that, for example, the details of an individual customer are stored only once;
        not for each sales order the customer places
        - Tables are managed and queried using Structured Query Language (SQL)
    
    - Non Relational Databases:
        - Don't apply a relational schema to the data
        - Often called as NoSQL database, even though some support variant of SQL
        - Four Common types of Non-Relational Database:
            - Key-Value Databases:
                - Each record consists of a unique key and an associated value
                - Can be in any format
            - Document Databases:
                - Specific form of key-value database in which the value is a JSON 
                document which the system is optimized to parse and query
            - Column Family Databases:
                - Stores Tabular data comprising rows and columns
                - You can divide the columns into groups known as column families
                - Each column holds a set of columns that are logically related
                togather
            - Graph Databases:
                - Stores entities as nodes with links to define relationships
                between them
    
    - Transactional Data Processing (OLTP):
        - Transaction is a small, discrete unit of work
        - Transactional systems are often high-volume, sometimes handling many 
        millions of transactions in a single day
        - The data being processed has to be accessible very quickly
        - The work performed by transactional systems is often referred to as 
        Online Transactional Processing (OLTP)
        - OLTP relies on a database system in which data storage is optimized for both
        read and write operations in order to support transactional workloads in which 
        data records are created, retrieved, updated and deleted (CRUD)
        - OLTP systems are typically used to support live applications that process
        business-data - often referred to as line of business applications
        - OLTP systems enforce transactions that support so-called ACID semantics:
            - Atomicity:
                - Each transaction is treated as a single unit, which suceeds completely
                or fails completely
                - Ex. Debiting and Crediting must be completed togather, if either
                action can't be completed than the other action must fail
            - Consistency:
                - Transactions can only take the data in the database from one valid
                state to another 
                - Ex. In debit and Credit, the completed state of the transaction must
                reflect the transfer of funds from one account to the other
            - Isolation:
                - Concurrent transactions cannot interfere with one another, and must
                result in a consistent database state
                - Ex. While transaction to transfer funds from one account to another
                in in-process, another transaction that checks the balance of these 
                accounts must return consistent results
                - W.r.t. above, the balance checking transaction can't retrieve a value
                for one account that reflects the balance before the transfer and a 
                value for the other account that reflects the balance after the transfer
            - Durability:
                - When a transaction has been committed, it will remain committed
                - Ex., After the account transfer transaction has completed, the revised
                account balances are persisted, so that even if the database system 
                were to be switched off, the committed transaction would be reflected,
                when it is switched on again
    
    - Analytical Data Processing (OLAP):
        - This type of data processing typically uses read-only (or read-mostly) 
        systems that stores vast volumes of historical data or business metrcis
        - OLAP model is an aggregated type of data storage that is optimized 
        for analytical workload
        - Ex. Find sales by region, by city, or for an individual address 
        - Analytics can be based on a snapshot of the data at a given point in
        time, or a series of snapshots
        - One common architecture for enterprise-scale analytics looks like:
            - Data files may be stored in a central data lake for analysis
            - An ETL process copies data from files and OLTP databases into a data
            warehouse that is optimized for read activity
            - Data in the data warehouse may be aggregated and loaded into an online
            analytical processing (OLAP) model, or cube
            - The data in the data lake, data warehouse and analytical model can be 
            queried to produce reports, visualizations and dashboards
        
        - Data Lakes 
            - Common in large-scale analytical processing scenarios, where
            a large volume of file-based data must be collected and analyzed
        - Data Warehouse:
            - Established way to store data in a relational schema that is optimized
            for read operations, primary queries to support reporting and data
            visualization
            - The data normalization schema may require some denormalization of data
            in an OLTP data source (introducing some duplication to make queries
            perform faster)

        - Since OLAP data is pre-aggregated, queries to return the summaries it
        contains can be run quickly

        - OALP for different types of users:
            - Data Scientists:
                - Work directly with data files in a data lake to explore and model
                data
            - Data Analysts:
                - Query tables directly in the data warehouse to produce complex
                reports and visualizations
            - Business Users:
                - Consume pre-aggregated data in an analytical model in the form of 
                reports or dashboards

Explore Data and Services:
    - Three Key Job Roles:
        - Database Administrators:
            - Responsible for Design, Implementation, Maintenance and Operational
            aspects of on-premises and cloud-based database systems
            - Responsible for the overall avaibility and consistent performance 
            and optimizations of databases
            - Works with stakeholders to implement policies, tools and processes
            for backup and recovery plans to recover following a natural disaster
            or human made error
            - Responsible for managing the security of the database, granting 
            privileges over the data, granting or denying to appropriate users
        - Data Engineer:
            - Collaborates with stakeholders to design and implement data-related
            workloads, including data ingestion pipelines
            - Involved in cleansing and transformation activities and data stores
            for analytical workloads
            - Uses Relational, Non Relational databases, file stores, and data streams
            - Responsible for ensuring the privacy of the data is maintained within
            the cloud and spanning from on-premises to the cloud data stores
            - Own the management and monitoring of data pipelines to ensure that data
            loads perform as expected
        - Data Analyst:
            - Enables Business to maximize the value of their data assets
            - Responsible for exploring data to identify trends and relationships,
            designing and building analytical models
            - Enables advanced analytics capabilities through reports and visualizations
    
    - Azure SQL:
        - Collective name for a family of relational database based on the Microsoft
        SQL database engine
        - Azure SQL Database:
            - Fully managed platform-as-a service (PaaS) database hosted in Azure
        - Azure SQL Managed Instance:
            - Hosted Instance of SQL server with automated maintenance
            - Allows more flexible configuration than Azure SQL DB, but with more
            adminitrative responsibility for the owner
        - Azure SQL VM:
            - A virtual machine with an installation of SQL server
            - Allows maximum configurability with full management responsibility
        - Database Administrators Responsibility:
            - Provision and Manage Azure SQL database systems to support line of
            business (LOB) applications that need to store transactional data
        - Data Engineers Responsibility:
            - Use Azure SQL database systems as sources for data pipelines that 
            performs extract, transform and load (ETL) operations 
            - ETL allows to ingest the transactional data into an analytical system
        - Data Analyst Responsibility:
            - Query Azure SQL databases directly to create reports
            - Although in large organizations, the data is generally combined with 
            data from other sources in an analytical data store to support 
            enterprise analytics
    
    - Azure Database for open-source managed relational databases:
        - Azure Database for MySQL:
            - Simple to use open source database management system 
            - Commonly used in Linux, Apache, MySQL and PHP (LAMP) stack apps
        - Azure Database for MariaDB:
            - Created by the original developers of MySQL
            - The database engine has been re-written and optimized to improve 
            performance
            - Offers compatibility with Oracle Database
        - Azure Database for PostgreSQL:
            - Hybrid Relational-object database
            - Store data in relational tables, but also enables you to store custom
            data types, with their own non-relational properties
        - Job for Database Admins, Data Engineers and Data Analysts remain similar
        as for Azure SQL 

    - Azure Cosmos DB:
        - Global Scale non relational (No SQL) database
        - Supports multiple Application Programming Interfaces
        - Store and Manage data as 
            - JSON documents
            - Key-Value Pairs
            - Column-families
            - Graphs
        - Database Administrators Responsibility:
            - Provision and Manage Cosmos DB instances
            - Software Developers may also manage NoSQL data storage as part of
            the overall application architecture
        - Data Engineers Responsibility:
            - Integrate Cosmos DB data sources into enterprise analytical solutions
        - Data Analysts Responsibility:
            - Modelling and Reporting from the warehouse where data came in 
            through Azure Cosmos DB
    
    - Azure Storage:
        - Blob Containers:
            - Scalable, cost effective storage for binary files
        - File Shares:
            - Network File share 
            - Typically found in corporate networks
        - Tables:
            - Key-value storage for applications that need to read and write 
            data values quickly
        - Data Engineers Responsibility:
            - Host Data Lakes - Blob Storage with a hierarchical namespace that 
            enables files to be organized in folders in a distributed file system
    
    - Azure Data Factory:
        - Enables you to define and schedule data pipelines to transfer and 
        transform data
        - Integrate pipelines with other Azure services, enabling you to ingest data
        from cloud data stores, process the data using cloud-based compute
        and persist the results in another data store
        - Data Engineers Responsibility:
            - Build ETL solutions that populate analytical data stores with data
            from transactional systems across the organization
    
    - Azure Synapse Analytics:
        - Comprehensive, unified data analytics solution that provides a single
        service interface for multiple analytical capabilities
        - Pipelines:
            - Based on the same technology as Azure Data Factory
        - SQL:
            - A highly scalable SQL engine, optimized for Data Warehouse workloads
        - Apache Spark:
            - Open Source Distributed processing system
            - Supports Multiple Programming Languages and APIs 
                - Java
                - Python
                - Scala
                - SQL
        - Azure Synapse Data Explorer:
            - High Performance Data Analytics solution
            - Optimized for real-time querying of log and telemetry data
            - Language used is Kusto Query Language (KQL)
        - Data Engineers Responsibility:
            - Create Unified data analytics solution
            - Combine Data Ingestion Pipelines, Data warehouse storage and data lake
            storage through a single service
        - Data Analysts Responsibility:   
            - Use SQL and Spark pools through interactice notebooks to explore and
            analyze data
            - Take advantage of integration with services such as Azure ML, Microsoft 
            Power BI to create data models and extract insights from the data

    - Azure Databricks:
        - Azure Integrated Version of the popular Databricks platform
        - Combines Apache Spark Data Processing Platform with SQL database semantics 
        and an integrated management interface to enable large-scale data analytics
        - Data Engineers Responsibility:
            - Create Analytical data stores in Azure Databricks
        - Data Analysts Responsibility: 
            - Use Native Notebook support to query and visualize data in an 
            easy to use web-based interface
    
    - Azure HDInsight:
        - It provides Azure Hosted Clusters for Popular Apache open-source
        big data processing technologies
        - Apache Spark
        - Apache Hadoop:
            - MapReduce Jobs Can be written in Java or Apache Hive, a SQL 
            based API that runs on Hadoop
        - Apache HBase:
            - An open source system for large scale NoSQL data storage and querying
        - Apache Kafka:
            - Message Broker for Data Stream processing
        - Data Engineers Responsibility:
            - Support big data analytics workloads that depends on multiple
            open source technologies
    
    - Azure Stream Analytics:
        - Real-time processing engine that captures a stream of data from an input,
        applies a query to extract and manipulate data from the input stream and
        writes the results to an output for analysis or further processing
        - Data Engineers Responsibility:
            - Incorporate Azure Stream Analytics into data analytics architectures
            that capture streaming data for ingestion into an analytical data store
            or for real-time visualization
    
    - Azure Data Explorer:
        - It is a standalone service 
        - Offers the same high-performance querying of log and telemetry data
        as the Azure Synapse Data Explorer runtime in Azure Synapse Analytics
    - Data Analysts Responsibility:
        - Query and Analyze data that includes a timestamp attribute, such as is 
        typically found in log files and Internet of things (IOT) telemetry data
    
    - Microsoft Purview:
        - Provides a solution for enterprise-wide data governance and discoverability
        - Allows to create a map of your data and track data lineage across multiple
        data sources and systems
        - Enables you to find trustworthy data for analysis and reporting
        - Data Engineer Responsibility:
            - Enforce Data Governance across the enterprise
            - Ensure the integrity of data used to support analytical workloads

    - Microsoft Power BI:
        - Platform for analytical data modelling and reporting
        - Data Analysts Responsibility:
            - Create and share interactive data visualizations