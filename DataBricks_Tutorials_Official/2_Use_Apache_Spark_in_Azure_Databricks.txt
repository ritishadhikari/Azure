Get to Know Spark:
    - The Azure Databricks service launches and manages Apache Spark clusters within
    your Azure Subscription
    - Apache Spark clusters are groups of computers that are treated as a single
    computer and handle the execution of commands issued from notebooks
    - Clusters enable processing of data to be parallelized across many computers
    to improve scale and performance
    - They consists of a Spark driver and worker nodes where the driver node sends 
    work to the worker nodes and instructs them to pull data from a specified
    source
    - The notebook interface is typically the driver program which contains the 
    main loop for the program and creates distributed datasets on the cluster, 
    then applies operations to those datasets
    - Driver Programs access Apache Spark through a Sparksession object regardless
    of deployment location
    - Work submitted to the cluster is split into as many independent jobs as 
    needed and this is how work is distributed across the cluster's nodes
    - Jobs are further divided into tasks 
    - The input to a job is partitioned into one or more partitions and these 
    partitions are the unit of work for each slot
    - The secret to Spark's high performance is parallelism where clusters 
    scale horizontally, adding new nodes to the clusters as needed
    - Spark parallelizes jobs at two levels:
        - The First level of parallelization is the executor - a Java Virtual 
        Machine (JVM) running on a worker node, typically one instance per node
        - The second level of parallelization is the slot - the number of which
        is determined by the number of cores and CPUs of each node
    - Each executor has multiple slots to which parallelized tasks can be assigned
    - The JVM is multithreaded but a single JVM has a finite upper limit. But by
    splitting the work into tasks, the driver can assign units of work to slots
    in the executors on worker nodes for parallel execution
    - The driver assigns a partition of data to each task so that each task knows
    which piece of data it is to process
    - Once started, each task will fetch the partition of data assigned to it
    - Spark breaks each job into stages to ensure everything is done in the right 
    order

Create a Spark Cluster:
    - A cluster mode can be:
        - Standard:
            - Suitable for single-user workloads that require multiple worker nodes
        - High Concurrency:
            - Suitable for workloads where multiple users will be using the cluster
            concurrently
        - Single Node:
            - Suitable for small workloads or testing, where a single worker node
            is required
    - When you create an Azure Databricks workspace, a Databricks appliance is 
    deployed in your subscription
    - When you create a cluster in the workspace, you specify the types and sizes
    of the virtual machines (VMs) to use for both the driver and the worker node,
    and some other configurations, but Azure Databricks manages all other aspects of
    the cluster
    - The Databricks appliance is deployed into Azure as a managed resource group
    (RG) within your subscription. This RG contains the driver and worker VMs for
    your clusters, along with other required resources including Virtual Networks,
    a Security Groups and a Storage account. 
    - All metadata for your cluster, such as scheduled jobs, is stored in an 
    Azure Database with geo-replication for fault tolerance
    - Azure Databricks internally uses Azure Kubernetes Service (AKS) to improve 
    Spark performance

Use Spark in Notebooks:
    - You can run many different kinds of application on Spark, including code
    in Python, or Scala Scripts, Java code compiled as a Java Archive (JAR) and 
    others
    - Spark is commonly used in two kinds of workloads:
        - Batch or Stream processing jobs to ingest, clean and transform data -
        often running as part of an automated pipeline
        - Interactive analytics sessions to explore, analyze and visualize data
    - Notebook consist of one or more cells, each containing either code or markdown
    - Code cells in notebooks have some features that can help you be more productive
    including:
        - Syntax Highlightning
        - Code auto-completion
        - Interactive data visualizations
        - The ability to export results

Use Spark to work with Data Files:
    - The default language in a new Azure Databricks Spark notebook is Pyspark,
    which is a spark optimized version of python
    - Additionally Scala and SQL can also be used
    - Software Engineers can also create compiled solutions that run on Spark using
    frameworks such as Java
    - Dataframes in Spark are similar to those in the ubiquitous Pandas Python library,
    but optimized to work in Spark's distributed processing environment
    ```
        # Display the first 10 rows
        %pyspark
        df=spark.read.load(
            "/data/products.csv",
            format="csv",
            header=True
        )
        display(df.limit(10))
    ```
    ```
    # Specify an explicit schema for the data, which is useful when the column names
    aren't included in the data file

    from pyspark.sql.types import *
    from pyspark.sql.functions import *

    productSchema=StructType([
        StructField("ProductID",IntegerType()),
        StructField("ProductName",StringType()),
        StructField("Category",StringType()),
        Structfield("ListPrice",FloatType())
        ])
    
    df=spark.read.load(
        "./data/product-data.csv",
        format="csv",
        schema=productSchema,
        header=False
    )

    display(df.limit(10))
    ```
    ```
    # Filtering DataFrames
    pricelistDF=df.select(
        "ProductID",
        "ListPrice"
        )

    # This also works
    pricelistDF=df[
        "ProductID",
        "ListPrice"
    ]
    ```
    ```
    # Select and where
    bikesDF=df.select(
        "ProductName",
        "ListPrice"
        ).\
        where(
            (
                df['Category']=="Mountain Bikes" 
            ) 
            |
            (
               df['Category']=="Road Bikes"
            )
        )
    ```
    ```
    # Group and Aggregate
    countsDF=df.select(
        "ProductID",
        "Category"
        ).groupBy(
            "Category"
        ).count()
    ```
    - The Dataframe API is part of a Spark library named SparkSQL, which enables data
    analysts to use SQL expressions to query and manipulate data
    - The Spark catalog is a metastore for relational data objects such as views and 
    tables and the spark runtime can use the catalog to seamlessly integrate code 
    written in any Spark-supported language with SQL expressions
    ```
        # Create Temporary View
        # A view is temporary and is automatically deleted at the end of the current
        # session
        df.createOrReplaceTempView("products")
    ```
    ```
        # Using SparkSQL API using a SQL Query to return data from the products 
        # view as a DataFrame
        bikesDF=spark.sql("""
            SELECT 
                productid,
                productname,
                listprice
            FROM 
                products
            WHERE
                category IN
                    ('Mountain Bikes",'Road Bikes')
        """
        )
    ```
    ```
        # Use %sql magic to run SQL code that queries objects in the catalogue
        %sql
        SELECT 
            Category, COUNT(ProductID) AS ProductCount
        FROM 
            products
        GROUP BY 
            Category
        ORDER BY 
            Category
    ```
    - The Matplotlib library requires data to be in Pandas Dataframe rather than a 
    Spark DataFrame, so the toPandas() method is used to convert it