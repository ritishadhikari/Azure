Use Automated Machine Learning in Azure Machine Learning:
    - Azure Machine Learning Workspace:
        - To use Azure Machine Learning, you first create a workspace resource in your Azure 
        Subscription
        - Use Workspace to manage data, compute resources, code, models and other artifacts related
        to your machine learning workloads
        - After creating Azure ML workspace, you can develop solutions with the Azure Machine 
        Learning service with developer tools or the Azure ML studio web portal

    - Azure Machine Learning Studio:
        - It is a web portal for Machine Learning Solutions in Azure
        - Includes a wide range of features and capabilities that help data scientists prepare data,
        train models, publish predictive services and monitor their usage   
        - To begin using ML Studio, you need to assign the workspace you created in Azure Portal to 
        Azure Machine Learning Studio

    - Azure Machine Learning Compute:
        - These are cloud based compute resources on which you can run model training and data
        exploration processes
        - In Azure ML Studio, you can manage the compure targets for Data Science Activities
        - There are four kinds of compute resources:
            - Compute Instances:
                - Development workstations that data scientists can use to work with data and models
            - Compute Clusters:
                - Scalable Clusters of VMs for On Demand processing of experiment code
            - Inference Clusters:
                - Deployment targets for predictive services that use your trained models
            - Attached Compute:
                - Links to existing Azure compute resources, such as Virtual Machines or Azure Data
                Bricks clusters

    - Azure Automated Machine Learning:
        - Azure ML includes an automated machine learning capability that automatically tries 
        multiple preprocessing techniques and model-training algorithms in parallel
        - It uses the power of cloud compute to find the best performing supervised ML model for 
        your data
        - For Data Scientists, it saves time and resources by automating algorithm selection and 
        hyperparameter tuning
        - It is also useful for people without extensive data science or programming knowledge 

    -  Understanding the AutoML Process:
        - The steps includes:
            - Prepare Data:
                - In Azure ML, data for model training and other operations is usually encapsulated
                in an object called a data asset
                - Create your own asset in Azure ML Studio
            - Train Model:
                - Use Automated ML to train models for:
                    - Classification
                    - Regression 
                    - Time Series Forecasting 
                    - Natural Language Processing
                    - Computer Vision
            - Evaluate Performance:
                - The best model is identified on the evaluation metric you specified
                - Cross Validation is used to calculate the evaluation metric
                - Deploy the best performing model as a service for client applications to use
            - Deploy a Predictive Service:
                - Deploy a service as an Azure Container Instances (ACI) (test) or to an Azure
                Kubernetes Cluster (AKS) (production)

Create a classification model with Azure Machine Learning designer:
    - You can use Azure Machine Learning Designer to create classification models by using a drag and
    drop visual interface, without needing to write any code

    - Azure Machine Learning Designer:
        - In Azure, one of many several ways to use a Classfication Machine Learning model is
        Azure Machine Learning Designer
        - It is a visual interface with drag and drop capability to train, test and deploy 
        ML models which can be shared, reused and version controlled
    
    - Pipelines:
        - Pipelines ley you organize, manage and reuse complex ML workflows across projects and users
        - A pipeline starts with dataset from which you want to train the model
        - Each time a pipeline is run, the configuration of the pipeline and its results are stored
        in your workspace as a pipeline job
    
    - Components:
        - A component encapsulates one step in a ML pipeline and is the building block for Azure ML
        Pipelines
    
    - Datasets:
        - Create Data assets from Data page from local files, a data store, web files and Open Datasets

    - Azure Machine Learning Jobs:
        - Azure ML Job executes a task against a specified compute target 
        - It enables a systemetic tracking for you ML experimentation and workflows
        - Once a job is created, Azure ML maintains a run record for the job
    
    - Steps for Classification in Azure Includes:
        - Prepare Data
            - Azure provides several pre-built components that can be used to prepare data for 
            training
            - These components enable you tp clean data, normalize features, join tables, etc
        - Train Model:
            - Dataset is required to train a classification model
            - You will use designer's Score component to generate the predicted class label value
        - Evaluate Performance
            - Azure provides many performance metrics and methodologies for evaluating how well a
            model makes predictions
        - Deploy a Predictive Service
            - Create and deploy an inference pipeline so as to automate your model into a service 
            that makes continous predictions 
            - Convert the training pipelines into a real-time inference pipeline. This
            process removes training components and adds web service inputs and outputs to handle 
            requests
            - After creating the inference pipeline, you can deploy it as an endpoint
            - In the endpoints page, you can view deployment details, test your pipeline service
            with sample data, and find credentials to connect your pipeline service to a client 
            application