Explore Azure Pipelines:
    - Azure Pipelines:
        - It is a fully featured service used to create cross-platform CI (Continous
        Integration) and CD (Continous Deployment)
        - It works with your preferred git provider and can deploy to most major cloud
        services, including Azure
        - It enables a constant flow of changes into production via an automated software
        production line
        - Each stage in a pipeline aims to verify the quality of new features from 
        a different angle to validate the new functionality and prevent errors from 
        affecting your users
        - This approach leads teams to continously monitor and learn where they are
        finding obstacles, resolve those issues, and gradually improve the pipeline's
        flow
        - A typical pipeline includes the following stages:
            - Build Automation and Continous Integration
                - New features implemented by the developers are integrated into the 
                central code base, built, and unit tested  
                - It is the most direct feedback cycle that informs the development team
                about the health of their application code
            - Test Automation:
                - The new version of an application is rigourously tested throughout 
                this stage to ensure that it meets all wished system qualities
                - All relevant aspects - functionality, security, performance or 
                compliance are verified by the pipeline
                - This stage may involve different types of automated or manual 
                activities
            - Deployment Automation:
                - It is a low risk step since the preceding stages have verified the 
                overall quality of the system
                - The deployment can be staged, with the new version being initially
                released to a subset of the production environment before being
                rolled out
                - The deployment is automated, allowing for the reliable delivery of 
                new functionality to users within minutes if needed
        - By carrying out value stream mappings on your releases, you can highlight any
        remaining inefficiencies and hotspots and pinpoint opportunities to improve
        your pipeline
        - These automated pipelines need infrastructure to run on and the efficiency of
        this infrastructure will have a direct impact on the effectiveness of the 
        pipeline
    
    - Languages Supported with Azure Pipelines:
        - Python
        - Java
        - PHP
        - Ruby
        - C#
        - Go
    
    - Before using CI/CD practices for your applications, you must have your source code
    in a version control system

    - Azure Pipelines Integrates with:
        - GitHub
        - GitLab
        - Azure Repos
        - Bitbucket
        - Subversion

    - Automated Platform Provisioning:
        - It ensures that your candidate applications are deployed to, and tests carried
        out against correctly configured and reproducible environments
        - It also helps horizontal scalability and allows the business to try out
        new products in a sandbox environment at any time
    
    - Deployment Targets for Azure Pipelines:
        - Container Registries
        - Virtual Machines
        - Cloud (AWS, GCP, Azure) or On-Premise
    
    - Items known as Artifacts are produced from CI systems. The continous delivery
    release pipelines use them to drive automatic deployments

    - Azure Pipelines Key Terms:
        - Agent:
            - When your build or deployment runs, the system begins one or more jobs
            - An agent is installable software that runs a build or deployment job
        - Artifact:
            - Collection of files and packages published by a build
            - They are made available for the tasks, such as distribution or deployment
        - Build:
            - It represents one execution of a pipeline
            - It collects the logs associated with running the steps and the test
            results
        - Continous Integration:
            - Simplifies the testing and building of code
            - Helps to catch bugs or problems early in the development cycle, making 
            more accessible and faster to fix
            - Automated tests and builds are run as part of this process
            - Process can run on a schedule or whenever code is pushed or both
            - Artifacts are produced from CI systems which the CD release pipelines
            use them to drive automatic deployments
        - Continous Delivery:
            - Code is built, tested and deployed to one or more test and production 
            stages since deploying and testing in multiple stages helps drive quality
            - Automated release pipelines consume artifacts (infrastructure and apps)
            to release new versions and fix existing systems
            - Monitoring and alerting systems constantly run to drive visibility into
            the entire CD process which ensure errors are caught often and early
        - Deployment Target:
            - A Virtual Machine, container, web app or any service used to host the 
            developed application
            - A pipeline might deploy the app to one or more deployment targets after
            the build is completed and tests are run
        - Job:
            - Represents an execution boundary of a set of steps where most jobs run
            on an agent and the steps run togather on the same agent
            - A build contains one or more jobs
        - Pipeline:
            - Made up of steps called tasks and defines the CI and CD process for 
            your app
            - Can be thought of as a script that describes how your test,build and 
            deployment steps are run
        - Release:
            - Made up of deployments to multiple stages
        - Stage:
            - Primary divisions in a pipeline
            - Ex. "Build the App", "Run Integration Tests", "Deploy to user acceptance
            testing"
        - Task:
            - Building Block of a pipeline
            - For Example, a Build Pipeline/stage might consist of build and test tasks
            and a Release Pipeline consists of deployment tasks
            - Each task runs a specific job in the pipeline
        - Trigger:
            - These are setup to tell the pipeline when to run
            - You can configure a pipeline to run upon a push to a repository at 
            scheduled times or upon completing another build
            - These actions are called Triggers

Manage Azure Pipelines Agents and Pools:
    - An agent is an installable software that runs one build or deployment job at a time
    - With a Microsoft hosted agent, maintenance and upgrades are automatically done
    - Each time a pipeline is run, a new virtual machine (instance) is provided, which
    is discarded after one use
    - A microsoft hosted agent has job time limits
    - Self Hosted Agent:
        - You set up and manage on your own to run build and deployment jobs
        - You can use in Azure Pipelines and it gives more control to install dependent
        software needed for your builds and deployments
        - Install the agent on:
            - Linux
            - macOS
            - Windows
            - Linux Docker containers
        - After installing the agent on a machine, you can install any other software 
        on that machine as required by your build or deployment jobs
        - A self-hosted agent does not have job limits
    
    - Job Types:
        - Agent Pool Jobs
            - The jobs run on an agent that is part of an agent pool
            - Instead of managing each agent individually, you organize agents into 
            agent pools
            - Agent pool defines the sharing boundary for all agents in that pool
            - In Azure Pipelines, pools are scoped to the entire organization so that 
            you can share the agent machines across projects
            - If you create an agent pool for a specific project, only that project can
            use the pool until you add the project pool into another project
            - When creating a build or release pipeline, you can specify which pool it
            uses, organization or project scope
            - To share an agent pool with multiple projects, use an organization scope
            agent pool 
        - Container Jobs
            - The jobs run in a container on an agent part of an agent pool
        - Deployment Group Jobs
            - Jobs that run on systems in a deployment group
        - Agentless Jobs 
            - Jobs that run directly on the Azure Devops
            - They don't require an agent for execution
            - It is also called as Server jobs 
    
    - Predefined Agent Pool:
        - Provides a pre-defined agent pool-named Azure Pipelines with Microsoft
        hosted agents
        - It lets you run jobs without configuring build infrastructure
        - VMs which are provided by default:
            - Windows Server 22 with VS Code 2022
            - Windows Server 19 with VS Code 2019
            - Ubuntu 20.04
            - Ubuntu 18.04
            - macOS 11
            - macOS X
        - By default, all contributors in a project are members of the User role on
        each hosted pool
        - This way, all contributors are allowed to build and release the pipelines
        using Microsoft hosted pool
    
    - Creating Agent Pools:
        - You are a project member, and you want to use a set of machines your team
        owns for running build and deployment jobs:
            - Make sure you are a member of a group in All Pools with the admin role
            - Create a new Project agent pool in your project settings and select the
            option to Create a new organization agent pool
            - As a result, both an organization and project-level agent pool will be
            created
            - Install and configure agents to be part of that agent pool
        - You are a member of the infrastructure team and would like to set up a 
        pool of agents for use in all projects:
            - Make sure you are a member of a group in All Pools with the admin role
            - Create a new organization agent pool in your admin settings and select
            AutoProvision corresponding project agent pools in all projects while
            creating the pool
            - This ensures all projects (existing or future) have a pool pointing to
            the organization agent pool
            - Install and configure agents to be part of that agent pool
        - You want to share a set of agent machines with multiple projects, but not all
        of them
            - Create a new Project agent pool in your project settings and select the
            option to Create a new organization agent pool
            - Go to each of the other projects, and create pool in each of them while
            selecting the option to Use an existing organization agent pool
            - Install and configure agents to be part of that agent pool
    
    - Communication of Agent with Azure Pipelines:
        - The agent communicates with Azure Pipelines to determine which job to run
        and reports the logs and job status
        - All the messages from the agent to Azure Pipelines over HTTPS depend
        on how you configure the agent
        - The user registers an agent with Azure Pipelines by adding it to an agent pool
        - You must be an agent pool admin to register an agent
        - The identity of the agent pool admin is needed only at the time of registration
        - It is not persisted or used to communicate further between the agent and 
        Azure Pipelines
        - Post the registration, the agent downloads a listener OAuth token and uses
        it to listen to the job Queue
        - Periodically the agent checks to see if a new job request has been posted
        in the job queue in Azure Pipelines
        - The agent downloads the job and a job-specific OAuth Token when a job is
        available. This job-specific OAuth Token is generated by Azure Pipelines for
        the scoped identity specified in the pipeline
        - The above token is short lived and is used by the agent to access resources
        (ex. source code) or modify resources (ex. upload test results) on Azure 
        Pipelines within that job
        - Once the job is completed, the agent discards the job-specific OAuth token
        and checks if there's a new job request using the listener OAuth token
        - The payload of the messages exchanged between the agent and Azure Pipelines
        are secured using Asymmetric encryption, where each agent has a public-private
        key pair and the public key is exchanged with the server during registration
        - The server uses the public key to encrypt the job's payload before sending
        it to the agent and the agent decrypts the job content using its private key
        - Secrets stored in build pipelines, release pipelines or variable groups 
        are secured when exchanged with the agent 
    
    - Communicate to deploy to target servers:
        - When you use the agents to deploy artifacts to a set of servers, it must-have
        "line of sight" connectivity to those servers
        - The microsoft hosted agent pools, by default, have connectivity to Azure
        websites and servers running in Azure
        - Incase your on-premise environments don't have connectivity to a microsoft
        hosted agent pool (because of intermediate firewall), you will need to manually
        configure self-hosted agents on the on-premise computer
        - The on-prem agents must have connectivity to the target on-premise environments
        and access to the internet to connect to Azure Pipelines or Azure Devops Server
        (see pic 8/13)
    
    - Agent can authenticate to Azure Devops using the following methods:
        - Personal Access Tokens (PAT):
            - Generate and use a PAT to connect an agent with Azure Pipelines
            - PAT is the only scheme that works with Azure Pipelines and is used only
            when registering an agent and not for succeeding communication
    
    - Once the Agent has been authenticated, you can run the agent either as a 
    service or an interactive process and you can chose which account you use to 
    run the agent
    - The choice of an agent account depends solely on the needs of the tasks running
    in your build and deployment jobs
    - For ex., to run tasks that use Windows authentication to access an external
    service, you must run the agent using an account with access to that service
    - After configuring the agent, it is recommended to try it in interactive
    mode to ensure it works
    - You can use the service manager of the OS to manage the lifecycle of the agent
    since the experience for auto-upgrading the agent is better when it's run as a 
    service
    - In some cases, you might need to run the agent interactively for production 
    use, such as UI tests where the autologon is enabled and screen-saver is disabled
    - Incase domain policies prevents you from enabling autologon or disabling
    screen saver, you may need to seek an exemption from the domain policy or run the 
    agent on a work-group computer where the domain policies don't apply

    - Agent Versions or Upgrades:
        - Microsoft updates the agent software every few weeks in Azure Pipelines
        - The agent version is indicated in the format {major}.{minor}
        - When a newer version of the agent is only different in minor versions, it 
        is automatically upgraded by Azure Pipelines when one of the tasks requires
        a newer version of the agent
        - If you run the agent interactively or a newer major version of the agent is
        available, you must manually upgrade the agents
        - The version can also be upgraded from the agent pools tab under your 
        project collection or organization
    
    - Security of Organization Agent Pool:
        - Reader:
            - View the Organization's agent pool and agents
            - Use it to add operators that are responsible for monitoring the 
            agents and their health
        - Service Account:
            - Use the Organization agent pool to create a project agent pool in a 
            project
        - Administrator:
            - Register or Unregister agents from the organization's agent pool
            - Refer to the organization agent pool when creating a project agent pool
            in a project
            - Manage membership for all roles of the organization agent pool
            - User that made the organization agent pool is automatically added to the
            administrator role for that pool
        - Others:
            - The All agent pools node in the Agent Pools tab is used to control the
            security of all organization agent pools
            - Role memberships for individual organization agent pools are automatically
            inherited from the "All agent pools" node

    - Security of Project Agent Pool:
        - Reader:
            - Members can view the project agent pool
            - Use it to add operators responsible for monitoring the build and 
            deployment jobs in that project agent pool
        - User:
            - Members can use the project agent pool when authoring build or release
            pipelines
        - Administrator:
            - Manage membership for all roles of the project agent pool
            - User that created the pool is automatically added to the administrator
            role for that pool
        - Others:
            - The All agent pools node in the Agent Pools tab is used to control the
            security of all project agent pools in a project
            - Role memberships for individual project agent pools are automatically
            inherited from the "All agent pools" node
            - By default the following groups are added to the admin role of All 
            agent pools:
                - Build Administrator
                - Release Administrator
                - Project Administrator

Describe pipelines and concurrency:
    - When an organization has only one Microsoft hosted parallel job, it
    allows users in that organization to collectively run only one
    build or release job at a time
    - When more jobs are triggered, they are queued and will wait for the
    previous job to finish
    - A release consumes a parallel job only when it is being actively 
    deployed to a stage
    - While the release is waiting for approval or manual intervention,
    it does not consume a parallel job
    - When a build or release runs, you can run multiple jobs as part of
    that build or release
    - Each job consumes a parallel job that runs on an agent. When there
    aren't enough parallel jobs available for your organization, then the
    jobs are queued up and run one after the other
    - You don't consume any parallel jobs when you run a server job or
    deploy to a deployment group
    
    - Determine how many parallel jobs you need:
        - You can begin by seeing if the free tier offered in your 
        organization is enough for your teams
        - When you have reached 1800 minutes per month limit for the free tier of
        Microsoft-hosted parallel jobs, you can start by buying one parallel job
        to remove this monthly time limit before deciding to purchase more
        - As the number of queued builds and releases exceeds the number of parallel
        jobs you have, your build and release queues will grow longer
        - When you find the queue delays are taking too long, you can purchase extra
        parallel jobs as needed
        - A simple rule of thumb is you will need one parallel job for every four 
        to five users in your organization

    - Detailed Estimate of the need for multiple parallel jobs:
        - If there are multiple teams, and if each of them requires a CI build,
        you will likely need a parallel job for each team
        - If your CI build trigger multiple branches, then each active branch would 
        need a parallel job
        - If you develop multiple applications by using one organization or server, 
        you will need one parallel jobs to deploy each application simultaneously
    
    - Sharing of Parallel Jobs across projects in a collection:
        - Parallel jobs are purchased at the organization level and they are shared
        by all projects in an organization
        - Currently there ain't a way to partition or dedicate parallel job capacity
        to a specific project or agent pool
    
    - Azure Pipelines with Open Source Projects:
        - With Public Projects, users can mark an Azure Devops Team project as Public
        - This will enable anonymous users to view the contents of that project
        in a read-only state enabling collaboration with anonymous (unauthenticated)
        users that was not possible before
    
    - Public Vs Private Projects:
        - Users that aren't signed into the service have read-only access to public 
        projects on Azure Devops
        - Private projects requires users to be granted access to the project and
        signed in to access the services
        - Microsoft will automatically apply the free tier limits for public projects
        if you meet both conditions:
            - Your pipeline is part of an Azure Pipelines public project
            - Your pipeline builds a public repository from Github or the same public
            project in your Azure Devops organization
    
    - Limits of Usage of Azure Pipelines:
        - You can have as many users you want when you are using Azure Pipelines and 
        there is no per-user charge for using Azure Pipelines
        - Users with both basic and stakeholder access can author as many builds and
        releases as they want
        - There are also no limit on the number of builds and release pipelines that
        you can create (definitions) for no charge and you can register any number of
        self-hosted agents for no cost
        - Visual Studio Enterprise subscribers get one self-hosted parallel job in each
        Azure Devops Services Organization where they are member
        - When you are running you builds for more than 14 paid hours in a month, then 
        per-minute plan might be less cost-effective than the parallel jobs model
    
    - Configure your build and release pipelines in the Azure Devops web portal 
    with the visual designer (Classic Pipelines):
        - It is great for new users in CI and CD
        - The visual representation of the pipeline makes it easier to get started
        - The visual designer is in the same hub as the build results which makes it
        easier to switch back and forth and make changes
    
    - Describing Azure Pipelines and YAML
        - You can either use YAML to define your pipelines or use the visual
        designer to do the same 
        - When using a Visual Designer, you define a build pipeline to build and test
        your code and publish artifacts; you can also specify a release pipeline
        to consume and deploy those artifacts to deployment targets
    
    - Azure Pipelines using YAML:
        - Configure Azure Pipelines to use your Git repo
        - Edit your azure-pipelines.yml file to define your build
        - Push your code to your version control repository. This action kicks off
        the default trigger to build and deploy and then monitor the results
        - Once code is updated, built, tested and packaged, it can be deployed to Target
    
    - Benefits of Using YAML:
        - The pipeline is versioned with your code and follows the same branching 
        structure. You get validation of your changes through code reviews in pull
        requests and branch build policies
        - Every branch you use can modify the build policy by adjusting the 
        azure-pipelines.yml file
        - A change to the build process might cause a break or result in an unexpected
        outcome. Because the change is in version control with the rest of your 
        codebase, you can more easily identify the issue

Explore Continous Integration:
    - Once you have your code in a version control system, you need an automated
    way of integrating the code on an ongoing basis
    - Azure Pipelines can be used to create a fully featured cross-platform CI and
    CD service
    - Continous Integration:
        - The Process of automating the build and testing of code every time a 
        team member commits changes to version control
        - CI encourages developers to share their code and unit tests by merging 
        their changes into a shared version control repository after every small
        task completion
        - Committing code triggers an automated build system to grab the latest code
        from the shared repository and build, test and validate entire main branch
        - In practice, continous integration relies on robust test suites and an 
        automated system to run those tests to address the friction of quality
        assurance mismatch within the integration process
        - When a developer merges code into the main repository, automated processes
        kick off a build of the new code
        - Afterward, test suites are run against the new build to check whether any
        integration problems were introduced
        - If either the build or the test phase fails, the team is alerted to work 
        to fix the build
        
        - Four Pillars of continous integration:
            - Version Control System
                - Manages changes to your source code over time
                - Ex. 
                    - Git
                    - Apache Subversion
                    - Team Foundation Version Control
            - Package Management System
                - Installs, Uninstalls and manages software packages
                - Ex. 
                    - Nuget
                    - Node Package Manager (npm)
                    - Chocolatey
                    - HomeBrew
                    - RPM
            - Continous Integration System  
                - Merges all developer working copies into a shared mainline several
                times daily
                - Ex.   
                    - Azure Devops
                    - TeamCity
                    - Jenkins
            - Automated Build Process
                - Creates software build, including compiling, packaging and running
                automated tests
                - Ex. 
                    - Apache Ant
                    - NAnt2
                    - Gradle
        - It removes long, complex and drawn-out bug-inducing merges, allowing 
        organizations to deliver swiftly
        - It also enables tracking metrics to assess code quality over time. Ex.
        Unit Test Passing rates, code that breaks frequently, code coverage trends 
        and code analysis
        - It can provide information on what has been changed between builds for
        traceability benefits
    
    - Build Number Formatting:
        - Build Number can be formatted by going to a build and then Options tab
    
    - Build Status:
        - Builds can be automatically triggered which is a key capability required
        for continous integration
        - But there are times that we might not want the build to run, even if it's 
        triggered; this can also be controlled by enabling/ pausing/ disabling builds
        to queue
    
    - Authorization and Timeout:
        - Authorization scope determines whether the build job is limited to accessing 
        resources in the current project, or accessing resources in the project 
        collection
        - The build job timeout determines how long the job can execute before being
        automatically cancelled where a value of zero specifies there's no limit
        - The build job cancel timeout determines how long the server will wait for
        a build job to respond to a cancellation request

Implement a Pipeline Strategy:
    - Configure Agent Demands:
        - There's a tab for capabilities on the Agent Pools page (at the organization
        level), when you select an agent
        - You can use it to see the available capabilities for an agent and to 
        configure user capabilities
        - When you configure a build pipeline and the agent pool to use, you can 
        specify specific demands that the agent must meet on the options tab
    
    - Implement Multi Agent Builds:
        - Use multiple build agents to support multiple build machines by either 
        distributing loads, run builds in parallel or use different agent capabilities
        - Adding Multiple Jobs to a pipeline lets you:
            - Break your pipeline into sections that need different pools or self-
            hosted agents
            - Publish artifacts in one job and consume them in one or more subsequent
            jobs
            - Build faster by running multiple jobs in parallel
            - Enable conditional execution of tasks
        - At the organizational level, you can configure the number of parallel jobs
        that are made available
        - The free tier allows for one parallel job of upto 1800 minutes per month;
        The self hosted agents have higher levels
        - You can define a build as a collection of jobs rather than as a single job
        - Each job consumes one of these parallel jobs that run on an agent
        - If there aren't enough parallel jobs available for your organization, the
        jobs will be queued and run sequentially
    
    - Azure Repos TFVC, Subversion and other generic git are not supported by Azure 
    Pipelines (YAML), while they are supported by Azure Pipelines (classic editor)

Integrate with Azure Pipelines:
    - Azure Pipelines can automatically build and validate every pull request
    and commit to your Azure Repos Git repository
    - Azure Pipelines can be used with Azure Devops public projects and Azure 
    Devops private projects

    - From the 3_helloWorldpipeline.yml file:
        - name:
            - It is in a build number format
            - You will get an monotonically increasing integer number if you don't 
            explicitly set a name format
            - This number is stored in Azure Devops
            - You can make use of this number by referencing $(Rev)
            - To make a date based number, use $(Date:yyyyMMdd) to get a build
            number like 20231102
            - To get a semantic number like 1.0.x, use 1.0.$(REV:.r)
        
        - trigger:
            - Most of triggers have been discussed in YAML
            - Trigger for any branch with a name that starts with topic:
                """
                    include:
                    - feature/*
                """
            - Pull requests can also filter branches and paths
            - Schedules allow you to specify cron expressions for scheduling pipeline
            runs
            - Pipelines will enable you to trigger pipelines when other pipelines
            are complete, allowing pipeline chaining
        
        - jobs:
            - It is a set of steps an agent executes in a queue (or pool)
            - They are atomic - performed wholly on a single agent
            - You may configure the same job to run on multiple agents simultaneously
            - You will need two jobs if you need some steps to run on one agent
            and some on another
            - Attributes of a Job:
                - displayName:
                    - A friendly name
                - dependsOn:
                    - A way to specify dependencies and ordering of multiple jobs
                    - By default they jobs run sequentially in the order in which
                    you define them in the yaml file
                    - The exception to this is when you add dependencies
                    - Pipelines must contain at least one stage with no dependencies
                    - To have multiple jobs run in parallel, we add dependsOn: []
                    to the jobs
                - condition:
                    - A binary expression
                    - If it evaluates to true, then the job runs, else job is skipped
                - strategy:
                    - Used to control how jobs are parralelized
                - continueOnError:
                    - Specify if the rest of the pipeline should continue if this
                    job fails
                - pool:
                    - The pool name (queue) to run this job on
                - workspace:
                    - Managing the source workspace
                - container:
                    - For specifying the container image to execute the job later
                - variables:
                    - Variables scoped to this job
                - steps:
                    - The set of steps to execute
                - timeoutInMinutes/ cancelTimeoutInMinutes:
                    - For controlling timeouts
                - services:
                    - sidecar services that you can spin up
        
        - checkout:
            - Unlike Classic Builds, pipelines require you to be more explicit using
            the checkout keyword
            - Jobs check out the repo they are contained in automatically unless
            you specify checkout: none
            - Deployment jobs don't automatically checkout the repo, so you will
            need to specify the checkout: self for deployment jobs if you want 
            access to the YAML file's repo
        
        - download:
            - For downloading Artifacts
            - Downloads work the opposite way for jobs and deployment jobs
            - Jobs don't download anything unless you explicitly define a download
            - Deployment jobs implicitly do a download: current, which downloads
            any pipeline artifacts created in the existing pipeline. To prevent it
            you must specify download: none
        
        - resources:
            - Resources lets you reference source code in 
                - Other repositories
                - pipelines
                - builds (classic builds)
                - containers (for container jobs)
                - packages
            - To reference code in another repo, specify that repo in the resources
            section and then reference it via its alias in the checkout step
        
        - steps:
            - The actual things that execute in the order specified in the job
        
        - variables:
            - Every variable is a key:value pair
            - The key is the variable's name and it has a value
            - To dereference a variable, wrap the key in $()

    - Pipeline Structure:
        - A pipeline is one or more stages that describes a CI/CD Process
        - Stages are primary divisions in a pipeline - "Build this app", "Run this
        test", "Deploy to preprod"
        - A stage is one or more jobs, units of work assignable to the same machine
        - You can arrange both stages and jobs into a dependency graphs. Ex. "Run
        this stage before that one", "This job depends on the output of that job"
        - If you have a single-stage, you can omit the stages keyword and directly
        specify the jobs keyword

        - Stage:
            - A stage is a collection of related jobs
            - By default, stages run sequentially, where each stage starts after
            the preceeding stage is complete
            - Use approval checks to control when a stage should run manually
            - The checks are commonly used to control deployments to production
            environments
            - Checks are a mechanism available to the resource owner and they control
            when a stage in a pipeline consumes a resource
        
        - Deployment Strategies:
            - Runonce:
                - It is the most straightforward deployment strategy in all the
                presented lifecycle hooks
            - Rolling: 
                - A rolling deployment replaces instances of the previous version
                of an application with instances of the new version
                - It can be configured by specifying the keyword rolling under the
                strategy: node
            - Canary:
                - You can roll out the changes to a small subset of servers
                - It is an advanced deployment strategy that helps mitigate the risk
                of rolling out new versions of applications
                - As you gain more confidence in the new version, you can release it
                to more servers in your infrastructure and route more traffic to it
        
        - Lifecycle Hooks:
            - Achieve deployment strategies technique by using lifecycle hooks
            - They inherit pool specified by the deployment job, where deployment
            jobs use the $(Pipeline.Workspace) system variable
            - Available Lifecycle Hooks:
                - preDeploy:
                    - Used to run steps that initialize resources before application
                    deployment starts
                - deploy:
                    - Used to run steps that deploy your application
                    - Download Artifact task will be auto-injected only in the deploy
                    hook for deployment jobs
                - routeTraffic:
                    - Used to run steps that serve the traffic to the updated version
                - postRouteTraffic:
                    - Run the steps after the traffic is routed
                    - These tasks monitor the health of the updated version for a 
                    defined interval
                - on: failure or on: success: Run steps for rollback actions 
                or clean-up
        
        - Steps:
            - It is a linear sequence of operations that make up a job
            - Each steps runs its process on an agent and access the pipeline 
            workspace on a local hard drive
            - Environment variables aren't preserved between steps, but file 
            system changes are

        - Tasks:
            - Tasks are the building blocks of a pipeline
            - There's a catalogue of tasks available to choose from
    
    - Azure Pipelines support Four types of Templates:
        - Stage
        - Job
        - Step
        - Variable
        - You can also use templates to control what is allowed in a pipeline
        and define how parameters can be used with parameter

    - You can define a set of steps in one file and use it multiple
    times in another

    - You can define a set of variables in one file and use it multiple
    times in other files

    - For more reference on templates explore:
        - https://learn.microsoft.com/en-us/training/
        modules/integrate-azure-pipelines/4-detail-templates

    - Use Multiple Repositories in your pipeline:
        - Repositories are first-class citizens within Azure Pipeline
        - Repositories can be specified as a repository resource or in 
        line with the checkout step
        - Supported repositories are Azure Repos Git
        - Github
        - Bitbucket Cloud
        - Checkout steps:
            - checkout: self
                - Current Repository is checkout out
            - checkout: none
                - No Repositories are synced or checked out
            - checkout: feature1
                - feature1 repository is checked out
            - no checkout steps:
                - checkout: self is the first step 
            - checkout: git://MyProject/MyRepo@features/tools
                -  feature/tools branch of git://MyProject/MyRepo
                is checked out as default
        - If your repository type requires a service connection or 
        other extended resources field, you must use a repository 
        resource

        - Azure Pipelines must be granted access to your repositories
        to trigger their builds and fetch their code during builds
        - The three authentication types for granting Azure Pipelines
        access to your Github Repositories while creating a pipelines:
            - GitHub App
            - OAuth
            - Personal Access Token (PAT)

Introduction to GitHub Actions:
    - Github actions are the primary mechanism for automation within GitHub
    - One of the most common is to implement Continous Integration

    - Actions:
        - They are the mechanism used to provide workflow automation within the 
        Github environment
        - They are defined in YAML and stay within Github repositories
        - Actions are executed on "runners", either hosted by Github or self-hosted
        - Contributed actions can be found in Github Marketplace
        - Actions can be used for wide variety of tasks such as:
            - Automated testing
            - Automatically responding to new issues
            - Triggering code reviews
            - Handling Pull requests
            - Branch Management 
        
    - Exploring Actions Flow:
        - Github tracks events that occur
        - Events can trigger the start of workflows
        - Workflows can also start on cron-based schedules and can be triggered by
        events outside of Github, or can be manually triggered
        - Workflows are the unit of automation and they contain jobs
        - Jobs use actions to get work done
        
    - Workflows:
        - It defines the automation required and details the events that should
        trigger the workflow
        - The job within a workflow defines the location in which the actions
        will run, like which runner to use
        - Workflows are written in YAML and live within a Github repository
        at the place .github/workflows

    - Syntax elements of a standard workflow:
        - name:
            - It is the name of the workflow
            - Optional but highly recommended
        - on:
            - Event or list of events that would trigger the workflow
        - jobs:
            - List of jobs to be executed
            - Contains one or more jobs
        - runs-on:
            - Tells actions which runner to use
        - steps:
            - List of steps for the job
            - Steps within a job execute on the same runner
        - uses:
            - Tells actions which predefined action needs to be retrieved 
            - Ex. you might have an action that installs node.js
        - run:
            - Tells the job to execute a command on the runner
            - Ex. you might execute an npm command

    - Events:
        - There are several types of events that can trigger workflow
        - Scheduled Events:
            - With this trigger a cron schedule needs to be provided
            """
                on: 
                    schedule:
                        - cron: '0 8-17 * * 1-5' # min hr dom m dow
            """
        - Code Events:
            - It occurs when an event of interest occurs in the repository
            """
                on:
                    pull_request
            """
        - Manual Events:
            - Unique event that is used to trigger workflow runs manually
            - Use the workflow_dispatch event
        - Webhook Events:
            - Executed when a Github webhook is called
            """
                # This event would fire, when someone updates a Wiki Page
                on:
                    gollum
            """
        - External Events:
            - Allows events to fire from external systems using repository_dispatch
    - Jobs:
        - Workflow contains one or more job
        - It is a set of steps that will be run in order on a runner
        - Steps within a job execute on the same runner and share the same file system
        - Logs produced by jobs are searchable and artifacts produced can be saved
        - By default, if a workflow contains multiple jobs, they run in parallel
        - You can define dependencies between jobs such that one job may need to 
        wait for another job to complete
    
    - Runners:
        - When you execute jobs, the steps execute on a Runner
        - The steps can be the execution of a shell script or the execution 
        of a predefined action
        - Github provides several hosted runners to avoid needing to spin up your 
        infrastructure to run actions
        - Maximum duration of a job is 6 hours and for a workflow is 72 hours
        - Apart from Javascript, if you need to use other languages, a Docker Container
        can be used
        - As of now, the docker container support is only linux based
        - Javascript actions will be faster (no container needs to be used) and more
        versatile runtime. Also Github UI is better for working with JavaScript actions
        - You can create self hosted runners, if you need different configurations to the
        ones provided
        - Self Hosted Runners allows you to customize the runner completely and you 
        then need to maintain (patch, upgrade) the runner system
        - Self hosted runners can be added at different levels within an enterprise:
            - Repository-level (single repository)
            - Organizational-level (multiple repositories in an organization)
            - Enterprise-level (multiple organizations across an enterprise)
        - Github strongly recommends that you do not use self hosted runners in public
        repos as it would be a significant security risk and would allow someone
        to run code on your runner within your network
    
    - Examine Release and Test an Action:
        - Actions will often produce console output and you don't need to connect
        directly to the runners to retrieve that output
        - The console output from actions is available directly from within the 
        Github UI's Actions tab
        - While you might want to retrieve the latest version of the action, there 
        are many situations where you might want a specific version of the action
        - You can request a specific release of action in several ways:
            - Tags:
                - Allow you to specify the precise versions that you want to work
                """
                    steps:
                        -uses: actions/install-timer@v2.0.1
                """ 
            - SHA-based hashes:
                - You can specify a requested SHA-based hash for an action
                - It ensures that the action has not changed
                - Downside is you won't receive updates to the action automatically
                either
                """
                    steps:
                        -uses: actions/install-timer@327239021f7cc39fe7327
                ""'
            - Branches:
                - Request actions by refering to the branch you would want to work with
                - Get the latest version from the branch
                - You will benefit from updates, but it also increases the chance of 
                code-breaking
                """ 
                    steps:  
                        -uses: actions/install-timer@develop
                """

Learn continuous integration with GitHub Actions:

    - Examine Environment Variables:
        - When using actions to create CI/CD workflows, you will typically need to pass
        variable values to the actions
        - It is done by using Environment variables
        - GitHub provides a series of built-in environment variables with (GITHUB_prefix)
        - Ex.
            - GITHUB_WORKFLOW: 
                - Name of the workflow
            - GITHUB_ACTION:
                - Unique identifier for the action
            - GITHUB_REPOSITORY:
                - Name of the repository 
                - Includes Name of the owner in owner/repo format

        - GITHUB_WORKFLOW:
            - name of the workflow
    
    - Sharing Artifacts between Jobs:
        - You will often need to pass artifacts created by one job to another
        - The most common ways to do it are upload-artifact and download-artifact
        actions
        - Upload Artifact 
            - This action uploads one or more files from your workflow to be shared 
            between jobs
            - You can upload a specific file
            """
              - uses: actions/upload-artifact
              with:
                name: harness-build-log
                path: bin/output/logs/harness.log
            """
            - You can upload an entire folder
            """
                - uses: actions/upload-artifact
                with:
                    name: harness-build-log
                    path: bin/output/logs/
            """
            - You can use wildcards:
            """
                - uses: actions/upload-artifact
                with:
                    name: harness-build-log
                    path: bin/output/logs/harness[ab]?/*
            """
            - You can specify multiple paths:
            """
                - uses: actions/upload-artifact
                with:
                    name: harness-build-log
                    path: |
                        bin/output/logs/harness.log
                        bin/output/logs/harnessbuild.txt
            """
        - Download Artifacts:
            - Action for downloading artifacts
            """
                - uses: actions/download-artifact
                with:
                    name: harness-build-log
                    path: bin/output/logs/
            """
            - If no path is specified, it is downloaded to the current directory
        - Artifact Retention:
            - A default retention period can be set for the repository, organization
            or enterprise
            - You can set a custom retention period when uploading, but it can't 
            exceed the defaults for the repository, organization or enterprise
            """
                - uses: actions/upload-artifact
                with:
                    name: harness-build-log
                    path: bin/output/logs/harness.log
                    retention-days: 12
            """
        - Deleting Artifacts:
            - Delete Artifacts directly in Github UI
    
    - Examine Workflow Badge:
        - Badges can be used to show the status of a workflow within a repository
        - They show if a repository is currently failing or passing
        - They typically get added to the README.md file for the repository
        - Badges are added by using URLs which is as follows:
            https://github.com/<OWNER>/<REPOSITORY>/actions/workflows/
            <WORKFLOW_FILE>/badge.svg
        - Badges usually indicate the status of the default branch, but can be 
        branch-specific which is done by adding a URL query parameter ?branch=<NAME>
    
    - Best Practices for creating actions:
        - Create chainable actions. Don't create large monolithic actions
        - Version your actions like other code since others might take dependencies
        on your versions of your actions
        - Provide the "latest" label
        - Add appropriate documentations
        - In the action.yml file, ensure it has been populated with author, icon,
        expected inputs and outputs
        - Consider contributing to the marketplace
    
    - Releases with Git Tags:
        - Release are software iterations that can be packed for release
        - In Git, releases are based on Git Tags which marks a point in the 
        history of the repository 
        - Often tags contains version numbers, but they can have other values
        - Tags can be viewed in the history of a repository
    
    - Encrypted Secrets:
        - Actions often use secrets within pipelines
        - Common usecases of secrets are passwords or keys
        - Secrets are similar to environment variables but encrypted
        - Secrets can be created at two levels:
            - Repository
            - Organization
        - If secrets are created at the organization level, access policies can limit
        the repositories that can use them
        - To create secrets for a repository, you must be the repository's owner
        - From the repository settings, chose Secrets, then New Secret
    
    - Using Secrets in a workflow:
        - Secrets aren't passed automatically to the runners when workflows are 
        executed
        - Instead when you include an action that requires access to a secret, you
        use the secrets context to provide it
        """
            steps:
                - name: Test Database Connectivity
                  with:
                    db_username: ${{ secrets.DBUserName }}
                    db_password: ${{ secrets.DBPassword }}
        """
        - Secrets shouldn't be passed directly as command-line arguments as they may
        be visible to others; Instead treat them like environment variables
        """
            steps:
                - shell: pwsh
                  env: 
                    DB_PASSWORD: ${{ secrets.DBPassword }}
                  run:  |
                    db_test "$env:DB_PASSWORD"
        """
        - A workflow can use upto 100 secrets and they are limited to 64 KB in size

Design a container build strategy:
    - Containers make highly use of the underlying infrastructure
    - You need to manage and connect containers to the outside world for scheduling,
    load balancing, and distribution
    - Container Orchestration tool like Azure Kubernetes Services (AKS) comes to 
    solve the above issue

    - AKS:
        - It is an open source system for deploying, scaling and managing
        containerized application
        - It handles the work of scheduling containers onto a compute cluster
        and manages the workloads to ensure they run as the user intended
    
    - Container:
        - It consists of an entire runtime environment
            - An application, plus all its dependencies
            - Libraries and other binaries
            - Configuration files needed to run it, bundled into one package
        - Unlike a VM, which provides hardware virtualization, a container provides
        operating-system-level virtualization by abstracting the "user-space"
        - Each container gets its isolated user space to allow multiple containers
        to run on a single host machine
        - OS level architecture is shared across containers
        - The only parts that are created from scratch are the bins and libs, and 
        this is what makes containers lightweight
    
    - Container Lifecycle:
        - Docker Build:
            - Create an Image by executing a Dockerfile
        - Docker Pull:
            - Retrieve the image, likely from a container registry
        - Docker Run:
            - Execute the container. An instance is created of the image
    
    - Multistage Dockerfiles:
        - Multistage builds give the benefits of the builder pattern without the
        hassle of maintaining n number of separate files
    
    - Considerations for multiple stage builds:
        - Try to avoid creating overly complex container images that couple togather
        several applications
        - Instead use multiple containers and try to keep each container to a single
        purpose
        - Breaking up application components into separate containers increases
        the chances of reusing containers
        - Also scaling up the application is possible
        - For Ex. In the web application, you might want to add replicas of the 
        website container but not for the database container
        - Start with an image that only contains packages that are required
        - It would be best to consider using docker volume support to maintain the
        isolation of your application and its data 
        - Volumes are persistent storage mechanisms that exist outside the lifespan
        of a container
    
    - Azure Container Instances (ACI):
        - ACI allows you to create your applications rather than provisioning and 
        managing the infrastructure that will run the applications
        - ACIs are simple and fast to deploy, and you gain the security of 
        hypervisor isolation for each container group
        - It ensures that your containers aren't sharing an OS kernel with other
        containers
    
    - Azure Kubernetes Services (AKS):
        - The de-facto standard for container orchestration
        - Lets you quickly deploy and manage Kubernetes to scale and run applications
        while maintaining overall solid security
    
    - Azure Container Registry (ACR):
        - Lets you store and manage container images in a central registry
        - It provides you with a Docker Private Registry as a first-class Azure 
        resource
        - All container deployments including DC/OS, Docker Swarm and Kubernetes are
        supported
        - The registry is integrated with other Azure Services like App Service,
        Batch, Service Fabric and others
        - It allows your devops team to manage the configuration of apps without being
        tied to the configuration of the target-hosting environment
    
    - Azure Container Apps:
        - Allows you to build and deploy modern apps and microservices using 
        serverless containers 
        - It deploys containerized apps without managing complex infrastructure
        - Scale dynamically based on HTTP traffic or events powered by Kubernetes
        Event Driven Autoscaling
        - Write code using your preferred programming language or framework and
        build microservices with full support for Distributed Application runtime
    
    - Azure App Service:
        - Provides managed service for both Windows and Linux-based web applications
        and provides the ability to deploy and run containerized applications
        for both platforms
        - Provides autoscaling and load balancing options and is easy to integrate
        with Azure Devops