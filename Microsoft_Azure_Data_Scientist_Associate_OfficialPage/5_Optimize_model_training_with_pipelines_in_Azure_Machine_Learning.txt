Run pipelines in Azure Machine Learning:
    - In an enterprise Data Science Project, you can train and retrain machine Learning models
    by running scripts as jobs
    - In an enterprise data science process, you will want to separate the overall process into
    individual tasks
    - You can group tasks togather as pipelines
    - Pipelines are key to implementing an effective Machine Learning Operations (MLOPS) solution
    in Azure
    - Create components of individual tasks, making it easier to reuse and share code.
    - Then combine components into an Azure Machine Learning Pipeline, which you will run as 
    a pipeline job
    - An Azure Devops or Azure Synapse Analytics pipeline can trigger an Azure  Machine Learning
    Pipeline
    
    - Components:
        - Allows you to create reusable scripts that can easily be shared across users within 
        the same Azure Machine Learning Workspace
        - Use of Components:
            - To build a pipeline
            - To share ready-to-go code
        - Consists of three parts:
            - Metadata:
                - Includes the component's name, version, etc
            - Interface:
                - Includes the expected input parameters (like a dataset or hyperparameter) and
                expected output (like metrics and artifacts)
            - Command, Code and Environment:
                - Specifies how to run the code
        - To create a component, you would need two files:
            - A script that contains the workflow you want to execute
            - An YAML file to define the metadata, interface, and command, code and environment
            of the component
        - Load YAML component with the following Python Code:
            """
                from azure.ai.ml import load_component
                parent_dir=""
                loaded_component_prep=load_component(source=parent_dir+"./prep.yml")
            """
        - Register a component:
            - Pre-requisite is script and the YAML file
            - To make components accessible to other users in the workspace, you can also 
            register components  to the Azure Machine Workspace
            - Register a components using the following code:
                """
                    prep=ml_client.components.create_or_update(entities=prepare_data_component)
                """
        - Each components can be run on a specific compute target
        - Each component is executed as a child job as a part of overall pipeline job
        - You can create the YAML file, or use the command_component() function as a decorator
        to create the YAML file
    
    - Pipeline:
        - A pipeline is a workflow of Machine Learning tasks in which each task is defined
        as component
        - Each component can be run on a specific compute target, making it possible to combine different
        types of processing as required to achieve an overall goal
        - An AZML pipeline is defined in a YAML file which includes the pipeline job name, 
        inputs, outputs and settings
        - Once you have built a component-based pipeline in Azure ML, you can run the workflow 
        as a pipeline job
        - A pipeline is defined in a YAML file, which you can also create using the @pipeline 
        function
        - Code to build a pipeline that first prepare the data, and then trains the model:
            """
                from azure.ai.ml.dsl import pipeline

                @pipeline()
                def pipeline_function_name(pipeline_job_input):
                    prep_data=loaded_component_prep(input_data=pipeline_job_input)
                    train_model=loaded_component_train(training_data=prep_data.outputs.output_data)
                    
                    return {
                        "pipeline_job_transformed_data":prep_data.outputs.output_data,
                        "pipeline_job_trained_model":train_model.outputs.model_output

                    }
            """
        - To pass a registered data asset as the pipeline job input, you can call the function you created
        with the data asset as input:
            """
                from azure.ai.ml import Input
                from azure.ai.ml.constants import AssetType

                pipeline_job=pipeline_function_name(
                    pipeline_job_input=Input(type=AssetType.URI_FILE,
                                            path="azureml:data:1"
                                    )
                )
            """
        
        - The result of running the @pipeline() function is an YAML file; it can be reviewed by printing
        the pipeline_job
            # print(pipeline_job)

        - Code to change the output mode for the pipeline job outputs:
            """
                pipeline_job.outputs.pipeline_job_transformed_data.mode="upload"
                pipeline_job.outputs.pipeline_job_trained_model.mode="upload
            """
        
        - Code to Set the default pipeline compute; When a compute isn't specified for a component, it will
        use the default compute instead
            # pipeline_job.settings.default_compute="aml-cluster"
        
        - Code to change the default datastore where all the outputs will be stored:
            # pipeline_job.settings.default_datastore="workspaceblobstore"
        
        - Once you have configured the pipeline, you are ready to run the workflow as a pipeline
        job; Code to run a pipeline job:
            """
            pipeline_job=ml_client.jobs.create_or_update(
                    entities=pipeline_job,
                    experiment_name="pipeline_job"
                    )
            """
        
        - If there's an issue with the configuration of the pipeline itself, you'll find more information
        in the outputs and logs of the pipeline job
        - If there's an issue with the configuration of a component, you'll find more information in the 
        outputs and logs of the child job of the failed component

        - To automate the retraining of a model, you can schedule a pipeline
        - To schedule a pipeline job, you will use the JobSchedule class to associate a schedule
        to a pipeline job
        - Out of many options to create schedules, one simple option is to use a time based schedule 
        using the RecurrenceTrigger Class with the following parameters
            - frequency:
                - Unit of time to describe how often the schedule fires
                - Value can be either minute, hour, day, week or month
            - interval:
                - Number of frequency units to describe how often the schedule fires
                - Value needs to be in integer
        - Code to create a schedule that fires every minute:
            """
                from azure.ai.ml.entities import RecurrenceTrigger
                
                schedule_name="run_every_minute"

                # Create a Trigger
                recurrence_trigger=RecurrenceTrigger(
                    frequency="minute",
                    interval=1
                )

                # Schedule the Pipeline
                from azure.ai.ml.entities import JobSchedule
                
                job_schedule=JobSchedule(
                    name=schedule_name,
                    trigger=recurrence_trigger,
                    create_job=pipeline_job
                )

                job_schedule=ml_client.schedules.begin_create_or_update(
                    schedule=job_schedule
                ).result(   )
            """
        
        - To delete a schedule, first we must disable it. Code:
            """ 
                ml_client.schedules.begin_disable(name=schedule_name).result()
                ml_client.schedules.begin_delete(name=schedule_name).result()
            """