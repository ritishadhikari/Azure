Run a training script as a command job in Azure Machine Learning:
    - When you write code to process data and train models, you want the code to be scalable, repeatable
    and ready for automation
    - Scripts are a better fit for production workloads
    - In Azure ML, you can run a script as a command job using Python SDK V2

    - To configure a command job to run a script, you will need to specify values for the following parameters:
        - code:
            - The Folder that includes the script to run
        - command:
            - Specifies which file to run
        - environment:
            - The necessary packages to be installed on the compute before running the command
        - compute:
            - The compute to use to run the command
        - display_name:
            - The name of the individual job
        - experiment_name:
            - The name of the experiment, the job belongs to
        
    - Command Job with the Python SDK (v2):
        """
            from azure.ai.ml import command

            job=command(
                code="./src",
                command="python train.py", # "python train.py --training_data=diabetes.csv"
                environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
                compute="aml-cluster",
                display_name="train-model",
                experiment_name="train-classification-model"
            )

            returned_job=ml_client.create_or_update(entity=job)
        """

    - Define Parameters in the Script:
        - To use parameters in the script, use a library such as argparse to read arguments
        passed to the script and assign them to variables
        - To pass parameter values to a script, you need to provide the argument value in the command 
    
    - Scripts are ideal for testing and automation in your production environment
        - Remove nonessential code
        - Refactor your code into functions
            """
                def main(csv_file):
                    # Reads the CSV File
                    df=get_data(csv_file)
                    # Does Train Test and Split
                    X_train,X_test,y_train,y_test=split_data(df)
                
                def get_data(path):
                    df=pd.read_csv(path)
                    return df
                
                def split_data(df):
                    X,y=(df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure',
                    'TricepsThickness', 'SerumInsulin','BMI','DiabetesPedigree',
                    'Age']].values, df['Diabetic'].values)
                    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=0)
                    return X_train, X_test, y_train, y_test


            """ 
        - Test your script in the terminal
            """
                python train.py
            """

    - Experiments:
        - All jobs with the same experiment name will be grouped under the same experiment
        - You can find an individual job using the specified display name
        - All inputs and outputs of a command job are tracked
        - You can review which command you specified, which compute was used and which environment
        was used to run the script on the specified compute
        

Track Model Training with MLFlow in Jobs:
    
    - Next to tracking models that are trained in notebooks, you can also use MLFlow to track 
    models in scripts
    - MLFlow is an open source platform that helps you to track model metrics and artifacts across
    platforms and is integrated with Azure Machine Learning
    - Use MLFlow togather with AzureML to run training scripts locally or in the cloud.
    
    - Two options to track ML jobs with MLFlow:
        - Enable AutoLogging with mlflow.autolog()
        - Use logging functions to track custom metrics using mlflow.log_*

    - We need to include the ml-flow and azureml-mlflow in the environment to ensure the pip packages 
    are installed on the compute before running the script
    - Create an environment by referring to a YAML file that describes the Conda Environment
        """
            name: mlflow-env
            channels:   
                - conda-forge
            dependencies:
                - python=3.8
                - pip
                - pip:
                    - numpy
                    - pandas
                    - scikit-learn
                    - matplotlib
                    - mlflow
                    - azure-mlflow
        """
    - Once the environment is defined and registered, make sure to refer to it when submitting a job


    - Enable Autologging in MLFlow to log (without anyone needing to specify what needs to be logged):
        - Parameters
        - Metrics
        - Model Artifacts
    
    - Python Code for autologging:
        """
            import mlflow
            mlflow.autolog()
        """
    
    -  Log Custom Metrics with MLFlow(use the MLflow command to store metric with the experiment run):
        - mlflow.log_param():
            - Log single key-value parameter.
            - Use this function for an input parameter you want to log
        - mlflow.log_metric():
            - Log single key-value metric. 
            - Value must be a number. 
            - Use this function for any output you want to store with the run
        - mlflow.log_artifact():
            - Log a file
            - Use this function for any plot you want to log, save as image file first
    
    - Add Mlflow to an existing training script:
        """
         import mlflow
         reg_rate=0.1
         mlflow.log_param(f"Regularization Rate: {reg_rate}")
        """
    
    - Submit the Job:
        - Finally submit the training script as a job in Azure ML
        - You need to make sure that the environment you refer to in the job includes the necesary packages,
        and the script describes which you want to log 

    - View the metrics in the Azure Machine Learning Studio:
        - Logged Metrics will show in the Overview and Metrics tabs
        - Plots that are logged as artifacts are shown under Images
        - Find other artifacts like model files under Outputs + logs
    
    - Retrieve metrics with MLFlow in a Notebook:
        - MLflow in a notebook allows more control over which runs you want to retrieve to compare
        - List Active Experiments:
            """
                experiments=mlflow.search_experiments(max_results=2)
                for exp in experiments:
                    print(exp.name)
            """
        - List archived Experiments:
            """
                from mlflow.entities import ViewType

                experiments=mlflow.search_experiments(view_type=ViewType.ALL)
                for exp in experiments:
                    print(exp.name)

            """
        - 
        - Retrieve Runs in a specific experiment:
            """
            mlflow.search_runs(exp.experiment_id)
            """
        - If you want to search across all the experiments in the workspace:
            """
                mlflow.search_runs(exp.experiment_id, search_all_experiments=True)
            """
        - By default, experiments are ordered descending by start_time, which is the time the 
        experiment was queued in Azure Machine Learning. Change this using order_by
            """
                mlflow.search_runs(exp.experiment_id, order_by=["start_time DESC"], max_results=2)
            """
        - You can also look for a run with a specific combination in the hyperparameters:
            """
                mlflow.search_runs(
                    exp.experiment_id, filter_string="params.num_boost_round=100", max_results=2
                )
            """
    - Metrics:   
        - Any plots that are logged as artifacts can be found under Images
        - The model assets that can be used to register and deploy the model are stored in the models
        folder under Outputs+logs

