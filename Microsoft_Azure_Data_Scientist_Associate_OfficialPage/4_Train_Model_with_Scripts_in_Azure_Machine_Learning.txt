Run a training script as a command job in Azure Machine Learning:
    - When you write code to process data and train models, you want the code to be scalable, repeatable
    and ready for automation
    - Scripts are a better fit for production workloads
    - In Azure ML, you can run a script as a command job using Python SDK V2

    - To configure a command job to run a script, you will need to specify values for the following parameters:
        - code:
            - The Folder that includes the script to run
        - command:
            - Specifies which file to run
        - environment:
            - The necessary packages to be installed on the compute before running the command
        - compute:
            - The compute to use to run the command
        - display_name:
            - The name of the individual job
        - experiment_name:
            - The name of the experiment, the job belongs to
        
    - Command Job with the Python SDK (v2):
        """
            from azure.ai.ml import command

            job=command(
                code="./src",
                command="python train.py",  # "python train.py --training_data=diabetes.csv"
                environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
                compute="aml-cluster",
                display_name="train-model",
                experiment_name="train-classification-model"
            )

            returned_job=ml_client.create_or_update(entity=job)
        """

    - Define Parameters in the Script:
        - To use parameters in the script, use a library such as argparse to read arguments
        passed to the script and assign them to variables
        - To pass parameter values to a script, you need to provide the argument value in the command 
    
    - Scripts are ideal for testing and automation in your production environment
        - Remove nonessential code
        - Refactor your code into functions
            """
                def main(csv_file):
                    # Reads the CSV File
                    df=get_data(csv_file)
                    # Does Train Test and Split
                    X_train,X_test,y_train,y_test=split_data(df)
                
                def get_data(path):
                    df=pd.read_csv(path)
                    return df
                
                def split_data(df):
                    X,y=(df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure',
                    'TricepsThickness', 'SerumInsulin','BMI','DiabetesPedigree',
                    'Age']].values, df['Diabetic'].values)
                    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=0)
                    return X_train, X_test, y_train, y_test


            """ 
        - Test your script in the terminal
            """
                python train.py
            """

    - Experiments:
        - All jobs with the same experiment name will be grouped under the same experiment
        - You can find an individual job using the specified display name
        - All inputs and outputs of a command job are tracked
        - You can review which command you specified, which compute was used and which environment
        was used to run the script on the specified compute
        

Track Model Training with MLFlow in Jobs:
    
    - Next to tracking models that are trained in notebooks, you can also use MLFlow to track 
    models in scripts
    - MLFlow is an open source platform that helps you to track model metrics and artifacts across
    platforms and is integrated with Azure Machine Learning
    - Use MLFlow togather with AzureML to run training scripts locally or in the cloud.
    
    - Two options to track ML jobs with MLFlow:
        - Enable AutoLogging with mlflow.autolog()
        - Use logging functions to track custom metrics using mlflow.log_*

    - We need to include the ml-flow and azureml-mlflow in the environment to ensure the pip packages 
    are installed on the compute before running the script
    - Create an environment by referring to a YAML file that describes the Conda Environment
        """
            name: mlflow-env
            channels:   
                - conda-forge
            dependencies:
                - python=3.8
                - pip
                - pip:
                    - numpy
                    - pandas
                    - scikit-learn
                    - matplotlib
                    - mlflow
                    - azure-mlflow
        """
    - Once the environment is defined and registered, make sure to refer to it when submitting a job

    - Enable Autologging in MLFlow to log (without anyone needing to specify what needs to be logged):
        - Parameters
        - Metrics
        - Model Artifacts
    
    - Python Code for autologging:
        """
            import mlflow
            mlflow.autolog()
        """
    
    -  Log Custom Metrics with MLFlow(use the MLflow command to store metric with the experiment run):
        - mlflow.log_param():
            - Log single key-value parameter.
            - Use this function for an input parameter you want to log
        - mlflow.log_metric():
            - Log single key-value metric. 
            - Value must be a number. 
            - Use this function for any output you want to store with the run
        - mlflow.log_artifact():
            - Log a file
            - Use this function for any plot you want to log, save as image file first
    
    - Add Mlflow to an existing training script:
        """
         import mlflow
         reg_rate=0.1
         mlflow.log_param(f"Regularization Rate: {reg_rate}")
        """
    
    - Submit the Job:
        - Finally submit the training script as a job in Azure ML
        - You need to make sure that the environment you refer to in the job includes the necesary packages,
        and the script describes which you want to log 

    - View the metrics in the Azure Machine Learning Studio:
        - Logged Metrics will show in the Overview and Metrics tabs
        - Plots that are logged as artifacts are shown under Images
        - Find other artifacts like model files under Outputs + logs
    
    - Retrieve metrics with MLFlow in a Notebook:
        - MLflow in a notebook allows more control over which runs you want to retrieve to compare
        - List Active Experiments:
            """
                experiments=mlflow.search_experiments(max_results=2)
                for exp in experiments:
                    print(exp.name)
            """
        - List archived Experiments:
            """
                from mlflow.entities import ViewType

                experiments=mlflow.search_experiments(view_type=ViewType.ALL)
                for exp in experiments:
                    print(exp.name)

            """
        - 
        - Retrieve Runs in a specific experiment:
            """
            mlflow.search_runs(exp.experiment_id)
            """
        - If you want to search across all the experiments in the workspace:
            """
                mlflow.search_runs(exp.experiment_id, search_all_experiments=True)
            """
        - By default, experiments are ordered descending by start_time, which is the time the 
        experiment was queued in Azure Machine Learning. Change this using order_by
            """
                mlflow.search_runs(exp.experiment_id, order_by=["start_time DESC"], max_results=2)
            """
        - You can also look for a run with a specific combination in the hyperparameters:
            """
                mlflow.search_runs(
                    exp.experiment_id, filter_string="params.num_boost_round=100", max_results=2
                )
            """
    - Metrics:   
        - Any plots that are logged as artifacts can be found under Images
        - The model assets that can be used to register and deploy the model are stored in the models
        folder under Outputs+logs

Perform hyperparameter tuning with Azure Machine Learning:
    - Sweep Job:
        - In Azure ML, tune hyperparameters by submitting a script as a sweep job
        - A sweep job will run a trial for each hyperparameter combination to be tested
        - Each trial uses a training script with parameterized hyperparameter values to train
        a model, and logs the performance metric achieved by the trained model

    - Search Space:
        - It is the set of hyperparameter values tried during hyperparameter tuning 
        - Type of Hyperparameters:
            - Discrete Hyperparameters:
                - Require Discrete Values 
                - Values are selected from a particular finite set of possibilities
                - Define as a python List
                     # Choice(values=[10,20,30])
                - Define as a Python Range
                    # Choice(values=range(1,10))
                - Define in a Tuple:
                    # Choice(values=range(30,50,100))
                - Also select discrete values from any of the following discrete distributions:
                    - QUniform(min_value, max_value, q)
                    - QLogUniform(min_value, max_value, q)
                    - QNormal(mu, sigma, q)
                    - QLogNormal(mu, sigma, q)
            - Continous Hyperparameters:
                - Continous Values
                - Infinite number of possibilities
                - Specify any of the following distribution types:
                    - Uniform(min_value, max_value):
                        - Returns a Value distributed between min_value and max_value
                    - LogUniform(min_value, max_value):
                        - Returns a value drawn according to exp(Uniform(min_value, max_value))
                        - Logarithm of the returned value is uniformly distributed
                    - Normal(mu, sigma):
                        - Returns a real value thats normally distributed with mean mu and stdev sigma
                    - LogNormal(mu, sigma):
                        - Returns a value drawn according to exp(Normal(mu, sigma))
                        - Logarithm of the returned value is normally distributed
    - Defining a Search Space:
        """
            from azure.ai.ml.sweep import Choice, Normal

            command_job_for_sweep=job(
                batch_size=Choice(values=[16,32,64])
                learning_rate=Normal(mu=10, sigma=3)
            )

        """     

    - Sampling Methods: 
        - The specific values used in a hyperparameter tuning run or sweep job depends on the 
        type of sampling used
        - Grid Sampling:
            - Tries every possible combination
            """
            from azure.ai.ml.sweep import Choice, Normal

            command_job_for_sweep=job(
                batch_size=Choice(values=[16,32,64])
                learning_rate=Normal(mu=10, sigma=3)
            )

            sweep_job=command_job_for_sweep.sweep(
                sampling_algorithm="grid"
            )
            """  


        - Random Sampling:
            - Randomly choses values from the search Space
            """
            from azure.ai.ml.sweep import Choice, Normal

            command_job_for_sweep=job(
                batch_size=Choice(values=[16,32,64])
                learning_rate=Normal(mu=10, sigma=3)
            )

            sweep_job=command_job_for_sweep.sweep(
                sampling_algorithm="random"
            )
            """
            - Sobol function adds a random sampling to make the results reproducible
             """
            from azure.ai.ml.sweep import RandomParameterSampling, Choice, Normal

            command_job_for_sweep=job(
                batch_size=Choice(values=[16,32,64])
                learning_rate=Normal(mu=10, sigma=3)
            )

            sweep_job=command_job_for_sweep.sweep(
                sampling_algorithm=RandomParameterSampling(seed=123, rule="sobol")
            )
            """
        - Bayesian Sampling:
            - Choses new values based on previous results
            - Only Choice, Uniform and QUniform parameter expressions can be used
            """
            from azure.ai.ml.sweep import Choice, Uniform

            command_job_for_sweep=job(
                batch_size=Choice(values=[16,32,64])
                learning_rate=Uniform(min_value=0.05, max_value=0.1)
            )

            sweep_job=command_job_for_sweep.sweep(
                sampling_algorithm="bayesian"
            )
            """
    
    - Configuring Early Termination:
        - Default way in a sweep job is to set a maximum number of trials
        - A more sophisticated approach is to stop a sweep job when newer models don't produce
        significant results 
        - To stop a sweep job based on the performance of the models, you can use an early
        termination policy
        - This policy can be especially beneficial when working with continous hyperparameters
        in your search space
        - Two Main Parameters to chose when you use an early termination policy:
            - evaluation_interval:
                - Specifies at which interval you want the policy to be evaluated
            - delay_evaluation:
                - Specifies when to start evaluating the policy
        - To determine the extent to which a model should perform better than the previous trials,
        there are three options for early termination:
            - Bandit Policy:
                - Use this policy to stop a trial if the target performance metric underperforms
                the best trial so far by a specified margin
                """
                    from azure.ai.ml.sweep import BanditPolicy
                    
                    sweep_job.early_termination=BanditPolicy(
                        slack_amount=0.2,  # Margin difference
                        delay_evaluation=5,
                        evaluation_interval=1
                    )
                """
                - You can also use a slack factor which compares the performance metric as a ratio
                rather than an absolute value
            - Median Stopping Policy:
                - Abandons the trials where the target performance metric is worse than the Median
                of the running averages for all trials
                """
                    from azure.ai.ml.sweep import MedianStoppingPolicy

                    sweep_job.early_termination=MedianStoppingPolicy(
                        delay_evaluation=5,
                        evaluation_interval=1
                    )
                """
            - Truncation Selection Policy:
                - It cancels the lowest performing X% of the trials at each evaluation interval
                based on the truncation_percentage value you specify for X
                """
                    from azure.ai.ml.sweep import TruncationSweepPolicy

                    sweep_job.early_termination=TruncationSweepPolicy(
                        evaluation_interval=1,
                        truncation_percentage=20,
                        delay_evaluation=4
                    )

                """
    
    - Process to tune Hyperparameters by running a sweep job:
        - Create a script for Hyperparameter tuning
        - Configure and run a sweep job
        - Monitor and review sweep jobs
    
    